{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d352eb48",
   "metadata": {},
   "source": [
    "\n",
    "2D Hide-and-Seek Multi-Agent Reinforcement Learning ST455 Project\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419ec1d-3fc9-45dd-8a8c-7f8164b960dd",
   "metadata": {},
   "source": [
    "_Authors: 37140, 49537, 49212_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615e2b0",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we explore a 2D adaptation of the popular 'hide and seek' game with inverted rules; only one hider to start with, and multiple seekers. The end goal would be to be the single hider remaining at the end of the episode. We modelled the development of this game on a grid world using multi-agent reinforcement learning (MARL), and we used techniques learnt in this course, as well as other resources, to 'train' hiders and seekers to improve as compared to their completely random initial states.\n",
    "\n",
    "We design a 2D $10×10$ grid world with one **hider** and four **seekers**, initialised with randomly placed obstacles at a certain 'frequency' percentage of randomness. We set this to be $10%$ and agents only observe their local neighbourhood depending on their role (Manhattan distances of either $2$ or $3$). Seekers can share information about new cells in their vision range or hider sightings, and they receive an intrinsic “curiosity” bonus for exploring unvisited locations. The hider, in turn, is rewarded for each time step it remains uncaught."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ef940-565b-4318-8dca-dadffbeabb49",
   "metadata": {},
   "source": [
    "### Rules of the Game:\n",
    "  1. Agents & Roles\n",
    "     - One “hider” and four “seekers” occupy free cells on the grid.\n",
    "     - The capturing seeker becomes the new hider (with short invisibility), and the former hider becomes a seeker upon capture.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  2. Grid & Obstacles\n",
    "     - Square grid of size $N \\times N$, with $N = 10$ with randomly placed walls.\n",
    "     - Agents may only occupy and move through non-wall cells.\n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  3. Movement\n",
    "     - At each time step, every agent chooses one of five actions: move up, down, left, right, or stay.\n",
    "     - Moves that would leave the grid, hit a wall, or collide with another agent are invalid, and the agent remains in place.\n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  4. Vision & Observation\n",
    "     - Seekers have a Manhattan distance vision range of $R = 2$: they observe all cells within $R$, including obstacles and any visible hider.\n",
    "     - If any seeker sees the hider who is not in its invisibility period, capture is immediate.\n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  5. Shared Belief & Communication\n",
    "     - Seekers share a central belief map over all grid cells.\n",
    "     - Belief is reset to full confidence when a sighting (but not a capture) occurs. It is broadcast to all seekers.\n",
    "     - Each sighting and each new cell exploration incurs a small communication cost per agent, deducted from the reward.\n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  6. Rewards\n",
    "     - Seekers:\n",
    "         1. +CaptureReward for catching the hider.\n",
    "         2. +ExploreReward for each newly discovered cell.\n",
    "         3. –TimePenalty each time step.\n",
    "         4. –CommCost per communication event.\n",
    "     - Hider:\n",
    "         1. +SurviveReward for each time step it remains uncaught.\n",
    "        <div style=\"margin-bottom:1em;\"></div>\n",
    "  7. Intrinsic Motivation\n",
    "     - Seekers track how often they visit each cell.\n",
    "     - Each visit grants a curiosity bonus: $\\beta / \\sqrt{\\text{visits} + 1}$, encouraging exploration of new areas.\n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "  8. Episode Termination\n",
    "     - The episode ends immediately when a maximum number of time steps (initialised as $100$) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4316790-f952-4c85-93af-3ee71a8ebdd8",
   "metadata": {},
   "source": [
    "To evaluate their performance, we track key metrics over 2,000 training episodes. We later provide a comparison with a different seed of experiments, which exhibit very useful information for analysis, from comparison to the main learning, as well as for evaluation of the quality of our implementation as a group. \n",
    "\n",
    "A quick summary of the initial learning we implemented during the project, as well as heatmap visualisations representing movement after 1000 episodes of learning are shown below, as well as the intuitive reasoning behind them. A detailed discussion and a critical analysis of the results are included in sections 5 and 6.\n",
    "\n",
    "<img src=\"training_outputs/plot_7.png\" width=\"900\"/>  \n",
    "We notice a lot of correlations between certain aspects of the game. For example, average role switches per episode follow episode reward variance very closely, which was seen with the more complex learning algorithm too. As well as this, we can see a decrease in action selection variance, which hinted towards some convergence with policy, which did appear somewhat weakly with our agents trained using Double Q Learning. \n",
    "\n",
    "<div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "We found in general that due to the nature of MARL games, convergence and proof of learning are not immediately explicit, and though certain aspects of learning were present, the use of higher-powered GPUs or longer running times would potentially have made outcomes more prominent. We observe that after initial noisy fluctuations, the long-term behaviours converge and remain stable by around 2000 training episodes.\n",
    "\n",
    "\n",
    "<div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "**Visit heatmaps after training**  \n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"training_outputs/training_output_1.png\" alt=\"Agent 0 Visits\" width=\"280\"/>\n",
    "  <img src=\"training_outputs/training_output_2.png\" alt=\"Agent 1 Visits\" width=\"280\"/>\n",
    "  <img src=\"training_outputs/training_output_3.png\" alt=\"Agent 2 Visits\" width=\"280\"/>\n",
    "</div>\n",
    "<div style=\"display: flex; gap: 10px; margin-top: 10px;\">\n",
    "  <img src=\"training_outputs/training_output_4.png\" alt=\"Agent 3 Visits\" width=\"280\"/>\n",
    "  <img src=\"training_outputs/training_output_5.png\" alt=\"Agent 4 Visits\" width=\"280\"/>\n",
    "</div>\n",
    "   After 1000 episodes, seekers exhibit distinct exploration patterns. For example, some agents focus on corridors near spawn points, while others explore more towards the edges of the maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bdbcd-27ef-40ac-8a0a-02bcd697489d",
   "metadata": {},
   "source": [
    "**Research questions:**  \n",
    "\n",
    "- *How does seekers' 'intrinsic curiosity' affect its coverage? How efficient is this with communication costs?*  \n",
    "- *Does Double Q or Actor–Critic yield stronger multi-agent coordination in this environment?*  \n",
    "\n",
    "We now describe the environment and data and implement our algorithms. We then present a rigorous numerical evaluation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa498cd-c3c0-4d14-939c-6dd9644b4340",
   "metadata": {},
   "source": [
    "# 2. Soundness of Solution Concepts \n",
    "\n",
    "We explore two key reinforcement learning methods: Double Q-learning and Actor-Critic. Alongside this, we extend our understanding of the solution concepts by implementing an enhanced reward structure, prioritised learning experience, target network updates for stable learning, shared belief maps, and intrinsic curiosity. We justify some of these choices below, with a more detailed implementation description provided in section 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387254f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**2.1 Double Q-Learning**  \n",
    "\"Double Q-learning decouples action selection from value estimation by maintaining two independent Q-networks, thereby mitigating the overestimation bias inherent in classical Q-learning\" (*van Hasselt et al., 2016*).  \n",
    "- *Rationale:* In partially observable hide-and-seek, noisy reward signals (possibly stemming from sporadic captures) can lead vanilla Q-learning to overvalue untested actions. Double Q-learning’s bias correction promotes more stable convergence (if convergence occurs).\n",
    "- *Analysis:* While bias reduction is beneficial, Double Q-learning can under-estimate values, potentially slowing down the exploitation of genuinely good policies. In practice, we observed slightly slower early convergence than vanilla Q, suggesting a trade-off between stability and sample efficiency.\n",
    "\n",
    "**2.2 Actor-Critic**  \n",
    "\"Actor-critic methods combine a parameterised policy (the Actor) with a learned value-function baseline (the Critic), yielding low-variance gradient estimates and natural handling of continuous or large action spaces\" (*Sutton et al., 2000*).  \n",
    "- *Rationale:* The hide-and-seek environment can involve continuous communication actions (e.g. broadcasting belief states) or movement choices. Policy gradients allow smooth adjustments of stochastic policies, improving exploration in high-dimensional action settings.  \n",
    "- *Critical Analysis:* Actor-critic methods suffer from high variance in gradient estimates and can be sensitive to hyperparameters (e.g. learning rates). In our experiments, tuning was more delicate than for Q-based agents, and some runs showed policy collapse.\n",
    "\n",
    "**2.3 Shared Belief Maps**  \n",
    "We implement a centralised “belief map” that aggregates partial observations from multiple agents into a common spatial estimate of opponent positions.  \n",
    "- *Rationale:* Partial observability hinders each agent’s situational awareness. By sharing beliefs, seekers can coordinate better avoiding duplicate searches and corner traps without requiring fully centralised control.  \n",
    "- *Critical Analysis:* While belief sharing improves team performance, it introduces communication overhead and a single point of failure: corrupted beliefs (due to sensor noise) can mislead all agents.\n",
    "\n",
    "**2.4 Intrinsic Curiosity Modules (ICM)**  \n",
    "\"ICM augments the external reward with an internal “novelty bonus” based on the prediction error of a learned forward model, encouraging agents to explore unfamiliar states\" (*Pathak et al., 2017*).  \n",
    "- *Rationale:* In tasks like hide-and-seek—where rewards are somewhat low and captures are infrequent, extrinsic learning can stagnate, and we noticed some phases of inaction where seekers too far from the hider would just bounce between $2$ adjacent squares. Curiosity rewards drive agents to cover the environment, discovering better hiding or seeking spots.  \n",
    "- *Critical Analysis:* Curiosity can lead to over-exploring irrelevant regions, especially in settings where novelty does not necessarily correlate with winning. We found that excessive intrinsic reward weight delayed convergence, so we had to focus on balancing of curiosity vs. task reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50367c",
   "metadata": {},
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "104dd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import unittest\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds all adjustable parameters for the hide-and-seek simulation.\n",
    "\n",
    "    Attributes:\n",
    "      seed: random seed for reproducibility\n",
    "      grid_size: size of the square grid world\n",
    "      num_agents: total agents (1 hider + seekers)\n",
    "      t_max: max time steps per episode\n",
    "      num_episodes: number of episodes\n",
    "      wall_prob: probability each cell is a wall\n",
    "      alpha, gamma: learning rate, discount factor\n",
    "      epsilon_*: parameters for epsilon-greedy\n",
    "      alpha_actor, alpha_critic: actor-critic learning rates\n",
    "      beta_intrinsic: weight for intrinsic (novelty) rewards\n",
    "      use_intrinsic: toggle for intrinsic rewards\n",
    "      capture_reward: reward when a seeker captures hider\n",
    "      survive_reward: reward per timestep for hider survival\n",
    "      explore_reward: reward seekers get for exploring new cells\n",
    "      timestep_penalty: small step penalty to encourage efficiency\n",
    "      comm_cost: cost per communication event\n",
    "      memory_span: not used directly but placeholder\n",
    "      belief_decay: decay rate for belief confidence\n",
    "      seeker_vision: vision radius for seekers\n",
    "      hider_vision: vision radius for hider (larger than seeker)\n",
    "      grace_steps: invisible steps after role switch\n",
    "      distance_reward: reward for maintaining optimal distance\n",
    "      role_switch_reward: reward for successful role switch\n",
    "      coordination_reward: reward for coordinated actions\n",
    "      alpha_decay: learning rate decay\n",
    "      alpha_min: minimum learning rate\n",
    "      replay_buffer_size: size of experience replay buffer\n",
    "      batch_size: batch size for experience replay\n",
    "      target_update_freq: frequency of target network updates\n",
    "    \"\"\"\n",
    "    seed: int = 42\n",
    "    grid_size: int = 10\n",
    "    num_agents: int = 5\n",
    "    t_max: int = 100\n",
    "    num_episodes: int = 2000\n",
    "    wall_prob: float = 0.1\n",
    "    alpha: float = 0.3\n",
    "    gamma: float = 0.95\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_min: float = 0.15\n",
    "    epsilon_decay: float = 0.9995\n",
    "    alpha_actor: float = 0.3\n",
    "    alpha_critic: float = 0.3\n",
    "    alpha_actor_decay: float = 0.9995\n",
    "    alpha_critic_decay: float = 0.9995\n",
    "    beta_intrinsic: float = 2.0\n",
    "    use_intrinsic: bool = True\n",
    "    capture_reward: float = 150.0\n",
    "    survive_reward: float = 3.0\n",
    "    explore_reward: float = 15.0\n",
    "    timestep_penalty: float = -0.2\n",
    "    comm_cost: float = 0.00\n",
    "    memory_span: int = 20\n",
    "    belief_decay: float = 0.95\n",
    "    seeker_vision: int = 1\n",
    "    hider_vision: int = 3\n",
    "    grace_steps: int = 3\n",
    "    distance_reward: float = 1.0\n",
    "    role_switch_reward: float = 30.0\n",
    "    coordination_reward: float = 8.0\n",
    "    alpha_decay: float = 0.9995\n",
    "    alpha_min: float = 0.05\n",
    "    replay_buffer_size: int = 20000\n",
    "    batch_size: int = 64\n",
    "    target_update_freq: int = 200\n",
    "\n",
    "\n",
    "# Initialise global configuration and random seeds\n",
    "tools_cfg = Config()\n",
    "random.seed(tools_cfg.seed)\n",
    "np.random.seed(tools_cfg.seed)\n",
    "\n",
    "# Define possible agent actions: (dx, dy)\n",
    "actions: List[Tuple[int, int]] = [\n",
    "    (0, 1),   # move right\n",
    "    (1, 0),   # move down\n",
    "    (0, -1),  # move left\n",
    "    (-1, 0),  # move up\n",
    "    (0, 0),   # stay in place\n",
    "]\n",
    "num_actions = len(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32de3fd",
   "metadata": {},
   "source": [
    "### SharedMemory Class\n",
    "\n",
    "The `SharedMemory` class provides centralised storage for seeker coordination, tracking explored cells, hider sightings, belief distributions, and communication metrics. It maintains the following state variables:\n",
    "\n",
    "1. **Grid Size**  \n",
    "   Integer `grid_size` defining the dimensions of the square grid world.\n",
    "\n",
    "2. **Explored Cells**  \n",
    "   A `set` of coordinates `explored` tracking cells visited by seekers.\n",
    "\n",
    "3. **Seeker Positions**  \n",
    "   A `set` of agent IDs `seeker_positions` tracking active seekers.\n",
    "\n",
    "4. **Last Sighting**  \n",
    "   Tuple `last_seen_pos` and integer `last_seen_time` recording the most recent hider observation.\n",
    "\n",
    "5. **Confidence**  \n",
    "   Float `confidence` measuring belief certainty, decaying over time.\n",
    "\n",
    "6. **Belief Distribution**  \n",
    "   Array `belief` of shape $(N,N)$ representing probability distribution over hider locations.\n",
    "\n",
    "7. **Communication Counters**  \n",
    "   Dictionaries `comm_count` and `total_comm_count` tracking message frequency.\n",
    "\n",
    "8. **Visit History**  \n",
    "   Dictionary `visit_count` mapping $(aid,pos)$ pairs to visit frequencies.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *_\\_init__(self, grid_size: int)*  \n",
    "   - Initializes `grid_size` and calls `clear_episode()` and `clear_step()`.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *clear_episode(self)*  \n",
    "   - Resets all episode-level state:\n",
    "     - `explored = set()`\n",
    "     - `seeker_positions = set()`\n",
    "     - `last_seen_pos = None`, `last_seen_time = -1`\n",
    "     - `confidence = 0.0`\n",
    "     - `belief = \\frac{1}{N^2}`\n",
    "     - `total_comm_count = 0`\n",
    "     - `visit_count = {}`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *clear_step(self)*  \n",
    "   - Resets `comm_count = {}` for the new time step.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *record_seeker(self, aid: int)*  \n",
    "   - Adds `aid` to `seeker_positions`.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "5. *record_sighting(self, aid: int, pos: Tuple[int,int], t: int, conf: float = 1.0)*  \n",
    "   - Ensures `aid` is in `seeker_positions`\n",
    "   - Increments `comm_count[aid]` and `total_comm_count`, then does likewise for all other seekers\n",
    "   - Sets `last_seen_pos = pos`, `last_seen_time = t`, `confidence = conf`\n",
    "   - Collapses `belief` to zero everywhere except `belief[pos]=1`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "6. *record_explored(self, aid: int, cells: Set[Tuple[int,int]]) → Set[Tuple[int,int]]*  \n",
    "   - Computes `new_cells = cells – explored`\n",
    "   - If nonempty, increments `comm_count[aid]`, `total_comm_count`, and updates `explored`\n",
    "   - Returns `new_cells`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "7. *record_visit(self, aid: int, pos: Tuple[int,int]) → int*  \n",
    "   - Increments and returns `visit_count[(aid,pos)]`\n",
    "   - Supports curiosity bonus of $\\frac{\\beta}{\\sqrt{\\text{visits}+1}}$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "8. *cost_and_reset(self, aid: int) → float*  \n",
    "   - Computes `cost = comm_count[aid] * \\text{CommCost}`\n",
    "   - Resets `comm_count[aid]=0`\n",
    "   - Returns `cost`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "9. *decay_confidence(self, t: int)*  \n",
    "   - If `last_seen_time<0` does nothing\n",
    "   - Else computes $\\Delta t = t - \\text{last\\_seen\\_time}$ and updates  \n",
    "     $$\n",
    "     \\gamma \\leftarrow \\gamma \\times (\\text{decay})^{\\Delta t}\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "10. *diffuse_belief(self)*  \n",
    "    - Rolls `belief` up/down/left/right, zeros out wrapped edges, averages all five arrays, then renormalises so $\\sum_{ij}B_{ij}=1$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "11. *get_belief(self, t: int, top_n: Optional[int] = None) → List[Tuple[Tuple[int,int],float]]*  \n",
    "    - Computes `weighted = (belief * confidence).flatten()`\n",
    "    - If `top_n` is `None` or $\\geq$ length, sorts all indices by descending weight\n",
    "    - Else uses partial sort on the top `top_n`\n",
    "    - Returns a list of $((i,j),\\,\\text{weight})$ pairs in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7105f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMemory:\n",
    "    \"\"\"\n",
    "    Centralised store for seeker coordination.\n",
    "    Tracks:\n",
    "      - Explored grid cells\n",
    "      - Last hider sightings (position + timestamp + confidence)\n",
    "      - Belief distribution over hider locations\n",
    "      - Communication counters\n",
    "      - Visit counts (for intrinsic rewards)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size: int):\n",
    "        self.grid_size = grid_size\n",
    "        self.clear_episode()\n",
    "        self.clear_step()\n",
    "\n",
    "    def clear_episode(self):\n",
    "        self.explored: Set[Tuple[int, int]] = set()\n",
    "        self.seeker_positions: Set[int] = set()\n",
    "        self.last_seen_pos: Optional[Tuple[int, int]] = None\n",
    "        self.last_seen_time: int = -1\n",
    "        self.confidence: float = 0.0\n",
    "        self.belief: np.ndarray = np.ones((self.grid_size, self.grid_size)) / (self.grid_size ** 2)\n",
    "        self.total_comm_count: int = 0\n",
    "        self.visit_count: Dict[Tuple[int, int], int] = {}\n",
    "\n",
    "    def clear_step(self):\n",
    "        self.comm_count: Dict[int, int] = {}\n",
    "\n",
    "    def record_seeker(self, aid: int):\n",
    "        self.seeker_positions.add(aid)\n",
    "\n",
    "    def record_sighting(self, aid: int, pos: Tuple[int, int], t: int, conf: float = 1.0):\n",
    "        if aid not in self.seeker_positions:\n",
    "            self.record_seeker(aid)\n",
    "        self.comm_count[aid] = self.comm_count.get(aid, 0) + 1\n",
    "        self.total_comm_count += 1\n",
    "        for sid in self.seeker_positions:\n",
    "            if sid != aid:\n",
    "                self.comm_count[sid] = self.comm_count.get(sid, 0) + 1\n",
    "                self.total_comm_count += 1\n",
    "        self.last_seen_pos = pos\n",
    "        self.last_seen_time = t\n",
    "        self.confidence = conf\n",
    "        self.belief.fill(0.0)\n",
    "        self.belief[pos] = 1.0\n",
    "\n",
    "    def record_explored(self, aid: int, cells: Set[Tuple[int, int]]) -> Set[Tuple[int, int]]:\n",
    "        new_cells = cells - self.explored\n",
    "        if new_cells:\n",
    "            self.comm_count[aid] = self.comm_count.get(aid, 0) + 1\n",
    "            self.total_comm_count += 1\n",
    "            self.explored |= new_cells\n",
    "        return new_cells\n",
    "\n",
    "    def record_visit(self, aid: int, pos: Tuple[int, int]) -> int:\n",
    "        key = (aid, pos)\n",
    "        self.visit_count[key] = self.visit_count.get(key, 0) + 1\n",
    "        return self.visit_count[key]\n",
    "\n",
    "    def cost_and_reset(self, aid: int) -> float:\n",
    "        cost = self.comm_count.get(aid, 0) * tools_cfg.comm_cost\n",
    "        self.comm_count[aid] = 0\n",
    "        return cost\n",
    "\n",
    "    def decay_confidence(self, t: int):\n",
    "        if self.last_seen_time < 0:\n",
    "            return\n",
    "        dt = t - self.last_seen_time\n",
    "        self.confidence *= (tools_cfg.belief_decay ** dt)\n",
    "\n",
    "    def diffuse_belief(self):\n",
    "        B = self.belief\n",
    "        B_up = np.roll(B, 1, axis=0)\n",
    "        B_down = np.roll(B, -1, axis=0)\n",
    "        B_left = np.roll(B, 1, axis=1)\n",
    "        B_right = np.roll(B, -1, axis=1)\n",
    "        B_up[0, :] = 0\n",
    "        B_down[-1, :] = 0\n",
    "        B_left[:, 0] = 0\n",
    "        B_right[:, -1] = 0\n",
    "        new_b = (B + B_up + B_down + B_left + B_right) / 5.0\n",
    "        total = new_b.sum()\n",
    "        if total > 0:\n",
    "            self.belief = new_b / total\n",
    "\n",
    "    def get_belief(self, t: int, top_n: Optional[int] = None) -> List[Tuple[Tuple[int, int], float]]:\n",
    "        weighted = (self.belief * self.confidence).flatten()\n",
    "        length = weighted.size\n",
    "        if top_n is None or top_n >= length:\n",
    "            indices = np.argsort(-weighted)\n",
    "        else:\n",
    "            partial = np.argpartition(-weighted, top_n)[:top_n]\n",
    "            indices = partial[np.argsort(-weighted[partial])]\n",
    "        result = []\n",
    "        for idx in indices:\n",
    "            i, j = divmod(idx, self.grid_size)\n",
    "            result.append(((i, j), weighted[idx]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f55cb0",
   "metadata": {},
   "source": [
    "### HideSeekEnv Class\n",
    "\n",
    "The `HideSeekEnv` class drives a hide-and-seek simulation on a grid, coordinating agent placement, movement, observations, captures and richly structured rewards. Internally it maintains:\n",
    "\n",
    "1. **Grid**  \n",
    "   An $N\\times N$ array $G\\in\\{0,1\\}^{N\\times N}$, sampled once on init via  \n",
    "   $$  \n",
    "   G_{ij}\\sim\\mathrm{Bernoulli}(1-\\text{wall\\_prob})  \n",
    "   $$\n",
    "\n",
    "2. **Shared Memory**  \n",
    "   A `SharedMemory` instance that tracks seekers' explored cells, last-seen hider location & confidence, belief diffusion, and communication counts.\n",
    "\n",
    "3. **Agents**  \n",
    "   A dict mapping `agent.id` → `AgentBase` (or subclass) objects, each with  \n",
    "   - position $(x,y)$  \n",
    "   - `role` (\"seeker\"/\"hider\")  \n",
    "   - invisibility counter  \n",
    "   - learning tables (Q-values or actor/critic weights)\n",
    "\n",
    "4. **Global Time**  \n",
    "   A counter $t$ of elapsed timesteps.\n",
    "\n",
    "5. **Switch Count**  \n",
    "   A counter $\\sigma$ of how many times a capture triggered a role swap.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self)*  \n",
    "   - Builds `self.grid` and instantiates `self.shared`.  \n",
    "   - Sets  \n",
    "     $$  \n",
    "     \\begin{aligned}\n",
    "     \\text{self.time}&=0,\\quad \\\\\n",
    "     \\text{self.switch\\_count}&=0,\\quad \\\\\n",
    "     \\text{self.agents}&=\\{\\}.  \n",
    "     \\end{aligned}\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *add_agent(self, agent, pos)*  \n",
    "   - Calls  \n",
    "     $$  \n",
    "     \\begin{aligned}\n",
    "     agent.set\\_initial\\_state&(x,y,role),\\quad \\\\\n",
    "     agent.env &= self\n",
    "     \\end{aligned}\n",
    "     $$  \n",
    "   - Inserts into `self.agents`.  \n",
    "   - If `agent.role=='seeker'`, calls `self.shared.record_seeker(agent.id)`.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *reset(self)*  \n",
    "   - Resets  \n",
    "     $$  \n",
    "     t\\leftarrow0,\\quad \\sigma\\leftarrow0  \n",
    "     $$  \n",
    "     and clears shared memory via `clear_episode()` and `clear_step()`.  \n",
    "   - Samples $|\\text{agents}|$ free cells $\\{(i,j)\\mid G_{ij}=0\\}$.  \n",
    "   - Randomly picks one agent as \"hider,\" the rest become \"seekers.\"  \n",
    "   - Resets each agent's invisibility and re-records seekers in shared memory.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *step(self, actions_dict) $\\to$ $\\{\\text{agent.id}\\mapsto r_i\\}$*  \n",
    "   Advances one timestep in **four phases**:\n",
    "\n",
    "   1. **Pre-move Capture**  \n",
    "      If a seeker is ℓ₁-adjacent to the hider, that seeker immediately captures:  \n",
    "      - All incur timestep penalty $-p_t$.  \n",
    "      - Capturing seeker gets  \n",
    "        $$  \n",
    "        R_{\\rm cap}+R_{\\rm role\\_switch}.  \n",
    "        $$  \n",
    "      - Roles swap, invisibility granted, $\\sigma\\!+\\!=1$, $t\\!+\\!=1$.  \n",
    "      - Return rewards.\n",
    "\n",
    "   2. **Belief & Sightings**  \n",
    "      - `shared.clear_step()`  \n",
    "      - `shared.decay_confidence(t)`  \n",
    "      - `shared.diffuse_belief()`  \n",
    "      - Each seeker observes; on sighting, calls  \n",
    "        `shared.record_sighting(id,(h_x,h_y),t)`.\n",
    "\n",
    "   3. **Movement Decision & Commit**  \n",
    "      - Propose each move $(x,y)\\to(x',y')$, invalid→stay.  \n",
    "      - Commit all moves, mark `moved_last_action`.  \n",
    "      - Seekers call  \n",
    "        $$  \n",
    "        \\text{shared.record\\_explored}(id,\\{\\text{new\\_pos}\\})  \n",
    "        $$  \n",
    "        to log newly explored cells.\n",
    "\n",
    "   4. **Reward Computation**  \n",
    "      For each agent $i$, start with  \n",
    "      $$  \n",
    "      r_i=-p_t.  \n",
    "      $$\n",
    "\n",
    "      - **Seeker**:  \n",
    "        1. Exploration: $+\\;R_{\\rm explore}\\times|\\Delta\\text{explored}|$. If no new cells, $-0.5$.  \n",
    "        2. Coordination: if `shared.last_seen_pos` exists with confidence > 0.5 and this move reduces the Manhattan distance to it, add $+\\;R_{\\rm coordination}$.\n",
    "\n",
    "      - **Hider**:  \n",
    "        1. Survival: $+\\;R_{\\rm survive}$.  \n",
    "        2. Distance-based: let  \n",
    "           $$  \n",
    "           d=\\min_{s\\in\\text{seekers}}\\|\\text{pos}_i-\\text{pos}_s\\|_1.  \n",
    "           $$  \n",
    "           If $2\\le d\\le4$, add $+\\;R_{\\rm distance}$; if $d<2$, subtract $R_{\\rm distance}$.\n",
    "\n",
    "      - **Stay penalty**: any agent whose `last_action==4` (stay) takes an extra $-1.0$.\n",
    "\n",
    "      Finally,  \n",
    "      $$  \n",
    "      t\\leftarrow t+1  \n",
    "      $$  \n",
    "      and return $\\{\\,i\\mapsto r_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "4bf73cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HideSeekEnv:\n",
    "    \"\"\"\n",
    "    Core simulator:\n",
    "      - Builds grid, places agents\n",
    "      - Manages moves, sightings, and captures\n",
    "      - Tracks rewards and role switches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid = (np.random.rand(tools_cfg.grid_size, tools_cfg.grid_size) < tools_cfg.wall_prob).astype(int)\n",
    "        self.shared = SharedMemory(tools_cfg.grid_size)\n",
    "        self.agents: Dict[int, 'AgentBase'] = {}\n",
    "        self.time = 0\n",
    "        self.switch_count = 0\n",
    "\n",
    "    def add_agent(self, agent: 'AgentBase', pos: Tuple[int, int]):\n",
    "        agent.set_initial_state(pos[0], pos[1], agent.role)\n",
    "        agent.env = self\n",
    "        self.agents[agent.id] = agent\n",
    "        if agent.role == 'seeker':\n",
    "            self.shared.record_seeker(agent.id)\n",
    "\n",
    "    def reset(self):\n",
    "        self.time = 0\n",
    "        self.switch_count = 0\n",
    "        self.shared.clear_episode()\n",
    "        self.shared.clear_step()\n",
    "        free_cells = [\n",
    "            (i, j)\n",
    "            for i in range(tools_cfg.grid_size)\n",
    "            for j in range(tools_cfg.grid_size)\n",
    "            if self.grid[i, j] == 0\n",
    "        ]\n",
    "        starts = random.sample(free_cells, tools_cfg.num_agents)\n",
    "        hider_id = random.choice(list(self.agents.keys()))\n",
    "        for agent in self.agents.values():\n",
    "            role = 'hider' if agent.id == hider_id else 'seeker'\n",
    "            x, y = starts.pop()\n",
    "            agent.set_initial_state(x, y, role)\n",
    "            agent.invisible = 0\n",
    "            if role == 'seeker':\n",
    "                self.shared.record_seeker(agent.id)\n",
    "\n",
    "    def step(self, actions_dict: Dict[int, int]) -> Dict[int, float]:\n",
    "        # Phase 0: preemptive capture detection (before any belief updates)\n",
    "        hider = next(a for a in self.agents.values() if a.role == 'hider')\n",
    "        finder = next(\n",
    "            (\n",
    "                s.id\n",
    "                for s in self.agents.values()\n",
    "                if s.role == 'seeker'\n",
    "                and abs(s.x - hider.x) + abs(s.y - hider.y) <= 1\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if finder is not None:\n",
    "            # Immediate capture: no belief decay, diffusion, or sightings\n",
    "            seeker = self.agents[finder]\n",
    "            rewards = {aid: tools_cfg.timestep_penalty for aid in self.agents}\n",
    "            rewards[finder] = tools_cfg.capture_reward + tools_cfg.role_switch_reward  # Add role switch reward\n",
    "            \n",
    "            # Role swap (no broadcast)\n",
    "            seeker.role, hider.role = 'hider', 'seeker'\n",
    "            if finder in self.shared.seeker_positions:\n",
    "                self.shared.seeker_positions.remove(finder)\n",
    "            self.shared.seeker_positions.add(hider.id)\n",
    "            seeker.invisible = tools_cfg.grace_steps\n",
    "            hider.invisible = 0\n",
    "            self.switch_count += 1\n",
    "            self.time += 1\n",
    "            return rewards\n",
    "\n",
    "        # Begin standard timestep updates\n",
    "        self.shared.clear_step()\n",
    "        self.shared.decay_confidence(self.time)\n",
    "        self.shared.diffuse_belief()\n",
    "\n",
    "        # Phase 1: observation (seekers see but don't catch)\n",
    "        for aid, agent in self.agents.items():\n",
    "            obs, seen = agent.observe(list(self.agents.values()), self)\n",
    "            if agent.role == 'seeker' and seen:\n",
    "                self.shared.record_sighting(aid, (hider.x, hider.y), self.time)\n",
    "\n",
    "        # Phase 2: check for capture (pre-move)\n",
    "        finder = None\n",
    "        for aid, agent in self.agents.items():\n",
    "            if agent.role == 'seeker' and abs(agent.x - hider.x) + abs(agent.y - hider.y) <= 1:\n",
    "                finder = aid\n",
    "                break\n",
    "        if finder is not None:\n",
    "            seeker = self.agents[finder]\n",
    "            rewards = {aid: tools_cfg.timestep_penalty for aid in self.agents}\n",
    "            rewards[finder] = tools_cfg.capture_reward + tools_cfg.role_switch_reward  # Add role switch reward\n",
    "            seeker.role, hider.role = 'hider', 'seeker'\n",
    "            if finder in self.shared.seeker_positions:\n",
    "                self.shared.seeker_positions.remove(finder)\n",
    "            self.shared.seeker_positions.add(hider.id)\n",
    "            seeker.invisible = tools_cfg.grace_steps\n",
    "            hider.invisible = 0\n",
    "            self.switch_count += 1\n",
    "            self.time += 1\n",
    "            return rewards\n",
    "\n",
    "        # Phase 3: decide all moves first\n",
    "        new_positions = {}\n",
    "        occupied = {agent.state() for agent in self.agents.values()}\n",
    "        for aid, agent in self.agents.items():\n",
    "            x0, y0 = agent.state()\n",
    "            action_index = actions_dict.get(aid, num_actions - 1)\n",
    "            agent.last_action = action_index\n",
    "            dx, dy = actions[action_index]\n",
    "            x1, y1 = x0 + dx, y0 + dy\n",
    "            if (\n",
    "                0 <= x1 < tools_cfg.grid_size\n",
    "                and 0 <= y1 < tools_cfg.grid_size\n",
    "                and self.grid[x1, y1] == 0\n",
    "                and (x1, y1) not in occupied\n",
    "            ):\n",
    "                new_positions[aid] = (x1, y1)\n",
    "            else:\n",
    "                new_positions[aid] = (x0, y0)\n",
    "\n",
    "        # Now update all positions and track exploration\n",
    "        new_explored = {aid: 0 for aid in self.agents}\n",
    "        final_occupied = set()\n",
    "        for aid, agent in self.agents.items():\n",
    "            new_pos = new_positions[aid]\n",
    "            agent.moved_last_action = (new_pos != agent.state())\n",
    "            agent.x, agent.y = new_pos\n",
    "            final_occupied.add(new_pos)\n",
    "            if agent.role == 'seeker':\n",
    "                newly = self.shared.record_explored(aid, {new_pos})\n",
    "                new_explored[aid] = len(newly)\n",
    "\n",
    "        # Phase 4: compute step rewards with enhanced structure\n",
    "        rewards: Dict[int, float] = {}\n",
    "        for aid, agent in self.agents.items():\n",
    "            cost = self.shared.cost_and_reset(aid)\n",
    "            r = tools_cfg.timestep_penalty - cost\n",
    "            \n",
    "            if agent.role == 'seeker':\n",
    "                # Exploration reward\n",
    "                r += tools_cfg.explore_reward * new_explored[aid]\n",
    "                if new_explored[aid] == 0:\n",
    "                    r -= 0.5  # Penalty for not finding new cells\n",
    "                \n",
    "                # Coordination reward\n",
    "                if self.shared.last_seen_pos and self.shared.confidence > 0.5:\n",
    "                    # Reward for moving towards the hider's last known position\n",
    "                    dx = self.shared.last_seen_pos[0] - agent.x\n",
    "                    dy = self.shared.last_seen_pos[1] - agent.y\n",
    "                    if abs(dx) + abs(dy) < abs(self.shared.last_seen_pos[0] - agent.x) + abs(self.shared.last_seen_pos[1] - agent.y):\n",
    "                        r += tools_cfg.coordination_reward\n",
    "            else:  # hider\n",
    "                r += tools_cfg.survive_reward\n",
    "                # Distance-based reward\n",
    "                seekers = [ag for ag in self.agents.values() if ag.role == 'seeker']\n",
    "                if seekers:\n",
    "                    min_dist = min(abs(agent.x - s.x) + abs(agent.y - s.y) for s in seekers)\n",
    "                    # Reward for maintaining optimal distance (not too close, not too far)\n",
    "                    if 2 <= min_dist <= 4:\n",
    "                        r += tools_cfg.distance_reward\n",
    "                    elif min_dist < 2:\n",
    "                        r -= tools_cfg.distance_reward  # Penalty for being too close\n",
    "            \n",
    "            # Penalty for staying still\n",
    "            if agent.last_action == 4:  # 'stay' action\n",
    "                r -= 1.0  # Strong penalty for staying\n",
    "                \n",
    "            rewards[aid] = r\n",
    "\n",
    "        self.time += 1\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "f52e9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Utility Function ----------------------\n",
    "def compute_bdir(dx: int, dy: int) -> int:\n",
    "    \"\"\"\n",
    "    Convert a 2D offset (dx, dy) into a discrete action index:\n",
    "      0: right, 1: down, 2: left, 3: up, 4: stay\n",
    "    Returns the most representative direction of the offset.\n",
    "    \"\"\"\n",
    "    if dx == 0 and dy == 0:\n",
    "        return num_actions - 1  # stay\n",
    "    if abs(dx) > abs(dy):\n",
    "        return 0 if dx > 0 else 2\n",
    "    return 1 if dy > 0 else 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bec28",
   "metadata": {},
   "source": [
    "### AgentBase Class\n",
    "\n",
    "The `AgentBase` class provides the structure shared by all agents which manages identity, position, role and bookkeeping. It maintains the following state variables:\n",
    "\n",
    "1. **Identifier**  \n",
    "   A unique integer `id`.\n",
    "\n",
    "2. **Coordinates**  \n",
    "   Current grid position $(x,y)$ stored in `x` and `y`.\n",
    "\n",
    "3. **Role**  \n",
    "   A string `role` equal to `\"seeker\"` or `\"hider\"`.\n",
    "\n",
    "4. **Invisibility counter**  \n",
    "   Integer `invisible` tracking how many steps the agent remains hidden after capture.\n",
    "\n",
    "5. **Cumulative reward**  \n",
    "   Float `reward_sum` summing all rewards earned in the current episode.\n",
    "\n",
    "6. **Environment reference**  \n",
    "   `env` pointing to the `HideSeekEnv` instance for observation and movement.\n",
    "\n",
    "7. **Visited set**  \n",
    "   A `set` of coordinates for debugging and analysis of exploratory behaviour.\n",
    "\n",
    "8. **Vision range**  \n",
    "   Integer `vision_range`, set to $2$ for seekers and $3$ for hiders, defining the radius of local observation.\n",
    "\n",
    "9. **Last action**  \n",
    "   Integer `last_action` tracking the most recent action taken.\n",
    "\n",
    "10. **Movement flag**  \n",
    "    Boolean `moved_last_action` indicating if the last action resulted in movement.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self, aid: int)*  \n",
    "   - Initializes agent with ID `aid`\n",
    "   - Sets default role based on ID (seeker if aid=0, hider otherwise)\n",
    "   - Initializes empty visited set and zero reward sum\n",
    "   - Sets vision range based on role (2 for seeker, 3 for hider)\n",
    "   - Initializes position to (0,0)\n",
    "   - Sets invisibility counter to 0  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *set_initial_state(self, x: int, y: int, role: str)*  \n",
    "   - Resets position to $(x,y)$, sets `role`, zeroes `invisible` and `reward_sum`, and clears `visited`.  \n",
    "   - Updates `vision_range` to $2$ if `role==\"seeker\"` else $3$.  \n",
    "   - Resets `last_action` to None and `moved_last_action` to False  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *state(self) → Tuple[int, int]*  \n",
    "   - Returns the agent's current coordinates as a tuple $(x,y)$.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *observe(self, agents: List['AgentBase'], env: HideSeekEnv) → (Dict[Tuple[int,int],Dict], bool)*  \n",
    "   - Scans all cells within `vision_range`, building  \n",
    "     ```python\n",
    "     obs: Dict[(i,j)→{\n",
    "       'obstacle': bool,\n",
    "       'contains_hider': bool\n",
    "     }]\n",
    "     ```  \n",
    "   - Sets `seen=True` if any observed cell contains a visible hider.  \n",
    "   - Returns `(obs, seen)`, supporting capture detection and learning updates without using policy logic.  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "5. *update(self, *args, **kwargs)*  \n",
    "   - Abstract method to be overridden by subclasses\n",
    "   - Handles learning updates based on experience\n",
    "   - Takes variable arguments to support different learning algorithms  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "6. *select_action(self, obs, shared: SharedMemory, t: int, agents: List[AgentBase]) → int*  \n",
    "   - Abstract method to be overridden by subclasses\n",
    "   - Determines the next action based on current state and observations\n",
    "   - Returns an integer representing the chosen action\n",
    "   - Updates `last_action` with the chosen action\n",
    "   - May update `moved_last_action` based on whether the action results in movement  \n",
    "    <div style=\"margin-bottom:1em;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "278fa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "    \"\"\"\n",
    "    Base agent with position, role, and bookkeeping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aid: int):\n",
    "        self.id = aid\n",
    "        self.role = 'seeker' if aid == 0 else 'hider'\n",
    "        self.visited = set()\n",
    "        self.reward_sum = 0\n",
    "        self.last_action = None  # Initialize last_action\n",
    "        self.moved_last_action = False  # Initialize moved_last_action\n",
    "        self.env = None\n",
    "        self.vision_range = 2 if self.role == 'seeker' else 3\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.invisible = 0\n",
    "\n",
    "    def set_initial_state(self, x: int, y: int, role: str):\n",
    "        self.x, self.y, self.role = x, y, role\n",
    "        self.invisible = 0\n",
    "        self.reward_sum = 0.0\n",
    "        self.visited.clear()\n",
    "        self.vision_range = 2 if role == 'seeker' else 3\n",
    "        self.last_action = None\n",
    "        self.moved_last_action = False\n",
    "\n",
    "    def state(self) -> Tuple[int, int]:\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    def observe(self, agents: List['AgentBase'], env: HideSeekEnv):\n",
    "        obs = {}\n",
    "        seen = False\n",
    "        r = self.vision_range\n",
    "        for dx in range(-r, r + 1):\n",
    "            for dy in range(-r, r + 1):\n",
    "                i, j = self.x + dx, self.y + dy\n",
    "                if 0 <= i < tools_cfg.grid_size and 0 <= j < tools_cfg.grid_size:\n",
    "                    contains = any(\n",
    "                        ag.id != self.id\n",
    "                        and ag.role == 'hider'\n",
    "                        and ag.invisible == 0\n",
    "                        and (ag.x, ag.y) == (i, j)\n",
    "                        for ag in agents\n",
    "                    )\n",
    "                    obs[(i, j)] = {\n",
    "                        'obstacle': env.grid[i, j] == 1,\n",
    "                        'contains_hider': contains,\n",
    "                    }\n",
    "                    seen = seen or contains\n",
    "        return obs, seen\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        \"\"\"Overridden by subclasses.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a53349-aa93-4509-9f81-767b1509b0fe",
   "metadata": {},
   "source": [
    "### Experience and PrioritizedReplayBuffer Classes\n",
    "\n",
    "The `Experience` and `PrioritizedReplayBuffer` classes work together to implement prioritised experience replay, a technique that improves learning efficiency by sampling more important experiences more frequently.\n",
    "\n",
    "The `Experience` class represents a single transition in the environment, storing:\n",
    "\n",
    "1. **State**  \n",
    "   Tuple `state` containing the agent's position $(x,y)$.\n",
    "\n",
    "2. **Action**  \n",
    "   Integer `action` representing the action taken.\n",
    "\n",
    "3. **Reward**  \n",
    "   Float `reward` received after taking the action.\n",
    "\n",
    "4. **Next State**  \n",
    "   Tuple `next_state` containing the resulting position $(x',y')$.\n",
    "\n",
    "5. **Done Flag**  \n",
    "   Boolean `done` indicating if the episode terminated.\n",
    "\n",
    "6. **Priority**  \n",
    "   Float `priority` used for importance sampling, defaulting to 1.0.\n",
    "\n",
    "---\n",
    "\n",
    "#### PrioritizedReplayBuffer Class\n",
    "\n",
    "The `PrioritizedReplayBuffer` class manages a collection of experiences with priority-based sampling. It maintains:\n",
    "\n",
    "1. **Capacity**  \n",
    "   Integer `capacity` defining the maximum number of experiences stored.\n",
    "\n",
    "2. **Priority Exponent**  \n",
    "   Float `alpha` controlling how much prioritization is used.\n",
    "\n",
    "3. **Importance Sampling Exponent**  \n",
    "   Float `beta` used to correct the bias introduced by prioritization.\n",
    "\n",
    "4. **Buffer**  \n",
    "   List `buffer` storing the experiences.\n",
    "\n",
    "5. **Priorities**  \n",
    "   Array `priorities` storing the priority values for each experience.\n",
    "\n",
    "6. **Position**  \n",
    "   Integer `position` tracking the current insertion point.\n",
    "\n",
    "7. **Size**  \n",
    "   Integer `size` tracking the current number of stored experiences.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *push(self, experience: Experience)*  \n",
    "   - Adds a new experience to the buffer\n",
    "   - If buffer is full, overwrites oldest experience\n",
    "   - Sets priority to maximum existing priority\n",
    "   - Updates position and size counters  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *sample(self, batch_size: int) → Tuple[List[Experience], np.ndarray, np.ndarray]*  \n",
    "   - Returns three elements:\n",
    "     1. List of sampled experiences\n",
    "     2. Array of sampled indices\n",
    "     3. Array of importance sampling weights\n",
    "   - If buffer size < batch_size, returns all experiences\n",
    "   - Otherwise, samples based on priorities:\n",
    "     $$  \n",
    "     P(i) = \\frac{p_i^\\alpha}{\\sum_j p_j^\\alpha}\n",
    "     $$\n",
    "   - Computes importance sampling weights:\n",
    "     $$  \n",
    "     w_i = \\left(\\frac{N \\cdot P(i)}{\\beta}\\right)^{-1}\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *update_priorities(self, indices: np.ndarray, priorities: np.ndarray)*  \n",
    "   - Updates priorities for experiences at given indices\n",
    "   - Used after computing TD errors to adjust sampling probabilities  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *_\\_len__(self) → int*  \n",
    "   - Returns current number of stored experiences  \n",
    "    <div style=\"margin-bottom:1em;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "064f69cf-37f1-4f51-b0b2-8cbc534c2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience:\n",
    "    \"\"\"Represents a single experience in the replay buffer.\"\"\"\n",
    "    state: Tuple[int, int]\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: Tuple[int, int]\n",
    "    done: bool\n",
    "    priority: float = 1.0  # Added priority field\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Stores and samples experiences with priority-based sampling.\"\"\"\n",
    "    def __init__(self, capacity: int = 20000, alpha: float = 0.6, beta: float = 0.4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Priority exponent\n",
    "        self.beta = beta    # Importance sampling exponent\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, experience: Experience):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[List[Experience], np.ndarray, np.ndarray]:\n",
    "        if self.size < batch_size:\n",
    "            return self.buffer, np.ones(len(self.buffer)), np.ones(len(self.buffer))\n",
    "            \n",
    "        # Calculate sampling probabilities\n",
    "        priorities = self.priorities[:self.size]\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Sample indices\n",
    "        indices = np.random.choice(self.size, batch_size, p=probs)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = (self.size * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        return [self.buffer[idx] for idx in indices], indices, weights\n",
    "\n",
    "    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):\n",
    "        self.priorities[indices] = priorities\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f91db6-2f2e-498e-88a7-d43f3a6c2bf6",
   "metadata": {},
   "source": [
    "### DoubleQLearningMixin Class\n",
    "\n",
    "The `DoubleQLearningMixin` gives an agent value-based learning off-policy (to mitigate overestimation bias). It maintains the following state variables:\n",
    "\n",
    "1. **First Q-table**  \n",
    "   A NumPy array $Q_{1}\\in\\mathbb{R}^{N\\times N\\times A}$ initialised to zeros.\n",
    "\n",
    "2. **Second Q-table**  \n",
    "   A NumPy array $Q_{2}$ of identical shape and initialisation.\n",
    "\n",
    "3. **Target Q-tables**  \n",
    "   NumPy arrays $Q_{1}^{\\text{target}}$ and $Q_{2}^{\\text{target}}$ for stable learning.\n",
    "\n",
    "4. **Exploration rate**  \n",
    "   Float $\\epsilon$, starting at `tools_cfg.epsilon_start` and decaying toward `tools_cfg.epsilon_min`.\n",
    "\n",
    "5. **Learning rate**  \n",
    "   Float $\\alpha$, starting at `tools_cfg.alpha` and decaying toward `tools_cfg.alpha_min`.\n",
    "\n",
    "6. **Replay Buffer**  \n",
    "   `PrioritizedReplayBuffer` instance for storing and sampling experiences.\n",
    "\n",
    "7. **Update Counters**  \n",
    "   Integer `steps_since_target_update` tracking when to update target networks.\n",
    "\n",
    "8. **Performance Tracking**  \n",
    "   List `episode_rewards` storing recent episode rewards for adaptive learning.\n",
    "\n",
    "9. **TD Error Tracking**  \n",
    "   List `td_errors` storing temporal difference errors for priority updates.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self)*  \n",
    "   - Initializes $Q_{1}$, $Q_{2}$, $Q_{1}^{\\text{target}}$, and $Q_{2}^{\\text{target}}$ to zero arrays\n",
    "   - Sets $\\epsilon \\gets \\mathit{epsilon\\_start}$ and $\\alpha \\gets \\mathit{alpha}$\n",
    "   - Creates `PrioritizedReplayBuffer` with capacity `tools_cfg.replay_buffer_size`\n",
    "   - Initializes empty lists for `episode_rewards` and `td_errors`\n",
    "   - Sets `steps_since_target_update = 0`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *decay_epsilon(self)*  \n",
    "   - Updates  \n",
    "     $$\n",
    "       \\epsilon \\;\\gets\\; \\max\\bigl(\\mathit{epsilon\\_min},\\,\\epsilon \\times \\mathit{epsilon\\_decay}\\bigr).\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *update_learning_rate(self, episode_reward: float)*  \n",
    "   - Appends `episode_reward` to `episode_rewards`\n",
    "   - If average reward over last 10 episodes is positive:\n",
    "     $$\n",
    "       \\alpha \\;\\gets\\; \\max\\bigl(\\mathit{alpha\\_min},\\,\\alpha \\times \\mathit{alpha\\_decay}\\bigr)\n",
    "     $$\n",
    "   - Otherwise:\n",
    "     $$\n",
    "       \\alpha \\;\\gets\\; \\min\\bigl(\\mathit{alpha},\\,\\alpha / \\mathit{alpha\\_decay}\\bigr)\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *select_action_double(self, x: int, y: int, bdir: int) → int*  \n",
    "   - Computes the element‐wise sum  \n",
    "     $$\n",
    "       Q_{\\text{sum}} = Q_{1}^{\\text{target}}[x,y] + Q_{2}^{\\text{target}}[x,y].\n",
    "     $$  \n",
    "   - Filters to actions that keep the agent within the grid.  \n",
    "   - With probability $\\epsilon$, returns a random valid action; otherwise returns  \n",
    "     $$\n",
    "       \\arg\\max_{a} \\;Q_{\\text{sum}}[a].\n",
    "     $$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "5. *double_q_update(self, ps: Tuple[int,int], a: int, ns: Tuple[int,int], r: float, bps: int, bns: int)*  \n",
    "   - Adds intrinsic reward for exploration:\n",
    "     $$\n",
    "       r \\;\\gets\\; r + \\frac{\\beta}{\\sqrt{\\text{visits}+1}}\n",
    "     $$\n",
    "   - Stores experience in replay buffer\n",
    "   - Updates target networks periodically:\n",
    "     $$\n",
    "       Q_{i}^{\\text{target}} \\;\\gets\\; Q_{i} \\quad \\text{if} \\quad \\text{steps} \\geq \\text{update\\_freq}\n",
    "     $$\n",
    "   - Samples batch from replay buffer\n",
    "   - With probability $0.5$, chooses one of two updates:  \n",
    "     - Update $Q_1$:  \n",
    "       $$\n",
    "       \\begin{aligned}\n",
    "         \\mathit{best} &= \\arg\\max_{a'} Q_{1}[ns,a'],\\quad \\\\\n",
    "         \\mathit{target} &= r + \\gamma\\,Q_{2}^{\\text{target}}[ns,\\mathit{best}],\\quad \\\\\n",
    "         \\mathit{td\\_error} &= |\\mathit{target} - Q_{1}[ps,a]|,\\quad \\\\\n",
    "         Q_{1}[ps,a] \\;&\\gets\\; Q_{1}[ps,a] + \\alpha\\,w\\,\\bigl(\\mathit{target} - Q_{1}[ps,a]\\bigr).\n",
    "       \\end{aligned}\n",
    "       $$  \n",
    "     - Or update $Q_2$ symmetrically using $Q_1$ for the target estimate\n",
    "   - Updates priorities in replay buffer based on TD errors\n",
    "   - Alternating which table provides evaluation versus target sharply reduces optimistic bias in single-table Q-learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "76c586e3-b00b-4f13-bf74-074fbb57fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningMixin:\n",
    "    \"\"\"\n",
    "    Adds Double Q-learning with prioritised experience replay, target networks, and adaptive learning rates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.Q1 = np.zeros((tools_cfg.grid_size, tools_cfg.grid_size, num_actions))\n",
    "        self.Q2 = np.zeros_like(self.Q1)\n",
    "        self.target_Q1 = np.zeros_like(self.Q1)\n",
    "        self.target_Q2 = np.zeros_like(self.Q2)\n",
    "        self.epsilon = tools_cfg.epsilon_start\n",
    "        self.alpha = tools_cfg.alpha\n",
    "        self.last_action = None\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(\n",
    "            capacity=tools_cfg.replay_buffer_size,\n",
    "            alpha=0.6,  # Priority exponent\n",
    "            beta=0.4    # Importance sampling exponent\n",
    "        )\n",
    "        self.update_target_every = tools_cfg.target_update_freq\n",
    "        self.steps_since_target_update = 0\n",
    "        self.batch_size = tools_cfg.batch_size\n",
    "        self.episode_rewards = []\n",
    "        self.td_errors = []  # Track TD errors for priority updates\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay the exploration rate.\"\"\"\n",
    "        self.epsilon = max(tools_cfg.epsilon_min, self.epsilon * tools_cfg.epsilon_decay)\n",
    "\n",
    "    def update_learning_rate(self, episode_reward: float):\n",
    "        \"\"\"Adapt learning rate based on performance.\"\"\"\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        if len(self.episode_rewards) > 10:\n",
    "            avg_reward = np.mean(self.episode_rewards[-10:])\n",
    "            if avg_reward > 0:\n",
    "                self.alpha = max(tools_cfg.alpha_min, self.alpha * tools_cfg.alpha_decay)\n",
    "            else:\n",
    "                self.alpha = min(tools_cfg.alpha, self.alpha / tools_cfg.alpha_decay)\n",
    "\n",
    "    def select_action_double(self, x: int, y: int, bdir: int) -> int:\n",
    "        valid = []\n",
    "        for i, (dx, dy) in enumerate(actions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if (0 <= nx < tools_cfg.grid_size and \n",
    "                0 <= ny < tools_cfg.grid_size and \n",
    "                self.env.grid[nx, ny] == 0):\n",
    "                valid.append(i)\n",
    "        \n",
    "        if not valid:\n",
    "            self.last_action = num_actions - 1\n",
    "            return self.last_action\n",
    "            \n",
    "        if random.random() < self.epsilon:\n",
    "            self.last_action = random.choice(valid)\n",
    "            self.decay_epsilon()\n",
    "            return self.last_action\n",
    "            \n",
    "        # Use target networks for action selection\n",
    "        qsum = self.target_Q1[x, y] + self.target_Q2[x, y]\n",
    "        masked_q = qsum.copy()\n",
    "        masked_q[~np.isin(np.arange(num_actions), valid)] = -float('inf')\n",
    "        self.last_action = int(np.argmax(masked_q))\n",
    "        return self.last_action\n",
    "\n",
    "    def double_q_update(self, ps: Tuple[int, int], a: int, ns: Tuple[int, int], r: float, bps: int, bns: int):\n",
    "        # Add intrinsic reward for exploration\n",
    "        if self.env and self.env.shared:\n",
    "            visit_count = self.env.shared.record_visit(self.id, ns)\n",
    "            r += tools_cfg.beta_intrinsic / np.sqrt(visit_count + 1)\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        done = False\n",
    "        experience = Experience(ps, a, r, ns, done)\n",
    "        self.replay_buffer.push(experience)\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        self.steps_since_target_update += 1\n",
    "        if self.steps_since_target_update >= self.update_target_every:\n",
    "            self.target_Q1 = self.Q1.copy()\n",
    "            self.target_Q2 = self.Q2.copy()\n",
    "            self.steps_since_target_update = 0\n",
    "        \n",
    "        # Sample from replay buffer and update Q-values\n",
    "        if len(self.replay_buffer) >= self.batch_size:\n",
    "            batch, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
    "            td_errors = []\n",
    "            \n",
    "            for i, exp in enumerate(batch):\n",
    "                if random.random() < 0.5:\n",
    "                    best_next = np.argmax(self.Q1[exp.next_state[0], exp.next_state[1], :])\n",
    "                    target = exp.reward + tools_cfg.gamma * self.target_Q2[exp.next_state[0], exp.next_state[1], best_next]\n",
    "                    td_error = target - self.Q1[exp.state[0], exp.state[1], exp.action]\n",
    "                    self.Q1[exp.state[0], exp.state[1], exp.action] += self.alpha * weights[i] * td_error\n",
    "                else:\n",
    "                    best_next = np.argmax(self.Q2[exp.next_state[0], exp.next_state[1], :])\n",
    "                    target = exp.reward + tools_cfg.gamma * self.target_Q1[exp.next_state[0], exp.next_state[1], best_next]\n",
    "                    td_error = target - self.Q2[exp.state[0], exp.state[1], exp.action]\n",
    "                    self.Q2[exp.state[0], exp.state[1], exp.action] += self.alpha * weights[i] * td_error\n",
    "                \n",
    "                td_errors.append(abs(td_error))\n",
    "            \n",
    "            # Update priorities based on TD errors\n",
    "            self.replay_buffer.update_priorities(indices, np.array(td_errors) + 1e-6)  # Add small constant to avoid zero priorities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9aaa4-7de4-458e-bf97-d85fbf064c85",
   "metadata": {},
   "source": [
    "### RLSeekerAgent Class\n",
    "\n",
    "The `RLSeekerAgent` combines `AgentBase` and `DoubleQLearningMixin` to implement a seeker guided by shared belief. It adds to the base agent:\n",
    "\n",
    "1. **Q-tables**  \n",
    "   Two arrays $Q_{1},Q_{2}\\in\\mathbb{R}^{N\\times N\\times A}$, initialised to zero.\n",
    "\n",
    "2. **Target Q-tables**  \n",
    "   Two arrays $Q_{1}^{\\text{target}},Q_{2}^{\\text{target}}$ for stable learning.\n",
    "\n",
    "3. **Exploration rate**  \n",
    "   Scalar $\\epsilon$, decayed after each selection.\n",
    "\n",
    "4. **Learning rate**  \n",
    "   Scalar $\\alpha$, adapted based on performance.\n",
    "\n",
    "5. **Vision range**  \n",
    "   Fixed to 2 for seekers.\n",
    "\n",
    "6. **Replay Buffer**  \n",
    "   `PrioritizedReplayBuffer` for storing and sampling experiences.\n",
    "\n",
    "7. **Performance Tracking**  \n",
    "   List `episode_rewards` for adaptive learning rate.\n",
    "\n",
    "8. **TD Error Tracking**  \n",
    "   List `td_errors` for priority updates.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self, aid: int)*  \n",
    "   - Calls both parent initialisers\n",
    "   - Sets `vision_range=2`\n",
    "   - Initializes Q-tables and target networks\n",
    "   - Creates prioritized replay buffer\n",
    "   - Sets up performance tracking lists  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *select_action(self, obs, shared: SharedMemory, t: int, agents)* → *int*  \n",
    "   - Fetches current state $(x,y)$\n",
    "   - Queries `shared.get_belief(t, top_n=1)` for the most likely hider cell $(i,j)$\n",
    "   - If belief exists and confidence > 0.2:\n",
    "     - Computes direction index $b$ toward $(i,j)$\n",
    "   - Otherwise:\n",
    "     - Finds nearest unexplored cell\n",
    "     - Computes direction index $b$ toward that cell\n",
    "   - Gets valid actions that keep agent within grid\n",
    "   - With probability $\\epsilon$:\n",
    "     - If valid actions exist toward preferred direction, chooses one\n",
    "     - Otherwise, chooses random valid action\n",
    "   - Otherwise:\n",
    "     - Computes $Q_{\\text{sum}} = Q_{1}^{\\text{target}}[x,y] + Q_{2}^{\\text{target}}[x,y]$\n",
    "     - Boosts Q-value for preferred direction by `tools_cfg.explore_reward`\n",
    "     - Masks invalid actions with $-\\infty$\n",
    "     - Returns $\\arg\\max_a Q_{\\text{sum}}[a]$\n",
    "   - Calls `decay_epsilon()`\n",
    "   - Returns the chosen action  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *update(self, ps, a, ns, r, bps, bns)*  \n",
    "   - Delegates to `double_q_update(ps,a,ns,r,bps,bns)`\n",
    "   - This method:\n",
    "     - Adds intrinsic reward for exploration\n",
    "     - Stores experience in replay buffer\n",
    "     - Updates target networks periodically\n",
    "     - Samples batch from replay buffer\n",
    "     - Updates Q-values using double Q-learning\n",
    "     - Updates priorities based on TD errors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "d6b48b99-51ac-4229-8e0a-5d3c853b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLSeekerAgent(AgentBase, DoubleQLearningMixin):\n",
    "    \"\"\"\n",
    "    Seeker uses Double Q-learning guided by shared belief.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aid: int):\n",
    "        AgentBase.__init__(self, aid)\n",
    "        DoubleQLearningMixin.__init__(self)\n",
    "\n",
    "    def select_action(self, obs, shared: SharedMemory, t: int, agents: List[AgentBase]) -> int:\n",
    "        ps = self.state()\n",
    "        belief = shared.get_belief(t, top_n=1)\n",
    "        # If we have a confident belief, chase the hider\n",
    "        if belief and belief[0][1] > 0.2:\n",
    "            ref = belief[0][0]\n",
    "            dx, dy = ref[0] - ps[0], ref[1] - ps[1]\n",
    "            bps = compute_bdir(dx, dy)\n",
    "        else:\n",
    "            # Fallback: move toward nearest unexplored cell\n",
    "            unexplored = [(i, j) for i in range(tools_cfg.grid_size)\n",
    "                          for j in range(tools_cfg.grid_size)\n",
    "                          if (i, j) not in self.env.shared.explored and self.env.grid[i, j] == 0]\n",
    "            if unexplored:\n",
    "                nearest = min(unexplored, key=lambda pos: abs(pos[0] - ps[0]) + abs(pos[1] - ps[1]))\n",
    "                dx, dy = nearest[0] - ps[0], nearest[1] - ps[1]\n",
    "                bps = compute_bdir(dx, dy)\n",
    "            else:\n",
    "                bps = num_actions - 1  # stay if nowhere to go\n",
    "\n",
    "        # Use the existing double Q-learning action selection, but bias toward bps\n",
    "        valid = []\n",
    "        for i, (adx, ady) in enumerate(actions):\n",
    "            nx, ny = ps[0] + adx, ps[1] + ady\n",
    "            if (0 <= nx < tools_cfg.grid_size and \n",
    "                0 <= ny < tools_cfg.grid_size and \n",
    "                self.env.grid[nx, ny] == 0):\n",
    "                valid.append(i)\n",
    "        if not valid:\n",
    "            self.last_action = num_actions - 1\n",
    "            return num_actions - 1\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            valid_towards = [i for i in valid if i == bps]\n",
    "            if valid_towards:\n",
    "                self.last_action = valid_towards[0]\n",
    "                self.decay_epsilon()\n",
    "                return self.last_action\n",
    "            self.last_action = random.choice(valid)\n",
    "            self.decay_epsilon()\n",
    "            return self.last_action\n",
    "\n",
    "        qsum = self.Q1[ps[0], ps[1]] + self.Q2[ps[0], ps[1]]\n",
    "        # Boost Q-value for the preferred direction\n",
    "        qsum[bps] += tools_cfg.explore_reward\n",
    "        masked_q = qsum.copy()\n",
    "        masked_q[~np.isin(np.arange(num_actions), valid)] = -float('inf')\n",
    "        self.last_action = int(np.argmax(masked_q))\n",
    "        return self.last_action\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        ps: Tuple[int, int],\n",
    "        a: int,\n",
    "        ns: Tuple[int, int],\n",
    "        r: float,\n",
    "        bps: int,\n",
    "        bns: int,\n",
    "    ):\n",
    "        self.double_q_update(ps, a, ns, r, bps, bns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87f287-5b56-4c20-b314-de94dd980cb5",
   "metadata": {},
   "source": [
    "### RLHiderAgent Class\n",
    "\n",
    "The `RLHiderAgent` also uses double Q-learning but chooses actions to evade. It differs only in vision and target selection:\n",
    "\n",
    "1. **Q-tables**  \n",
    "   Two arrays $Q_{1},Q_{2}\\in\\mathbb{R}^{N\\times N\\times A}$, initialised to zero.\n",
    "\n",
    "2. **Target Q-tables**  \n",
    "   Two arrays $Q_{1}^{\\text{target}},Q_{2}^{\\text{target}}$ for stable learning.\n",
    "\n",
    "3. **Exploration rate**  \n",
    "   Scalar $\\epsilon$, decayed after each selection.\n",
    "\n",
    "4. **Learning rate**  \n",
    "   Scalar $\\alpha$, adapted based on performance.\n",
    "\n",
    "5. **Vision range**  \n",
    "   Fixed to 3 for hiders.\n",
    "\n",
    "6. **Replay Buffer**  \n",
    "   `PrioritizedReplayBuffer` for storing and sampling experiences.\n",
    "\n",
    "7. **Performance Tracking**  \n",
    "   List `episode_rewards` for adaptive learning rate.\n",
    "\n",
    "8. **TD Error Tracking**  \n",
    "   List `td_errors` for priority updates.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self, aid: int)*  \n",
    "   - Initialises parents and sets `vision_range=3`\n",
    "   - Initialises Q-tables and target networks\n",
    "   - Creates prioritised replay buffer\n",
    "   - Sets up performance tracking lists  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *select_action(self, obs, shared, t, agents)* → *int*  \n",
    "   - Fetches current state $(x,y)$\n",
    "   - Gets valid actions that keep agent within grid\n",
    "   - If no valid actions exist, returns stay action\n",
    "   - Finds nearest seeker by Manhattan distance\n",
    "   - If seekers exist and nearest is within distance 3:\n",
    "     - Computes direction $b$ away from nearest seeker\n",
    "     - With probability $\\epsilon$:\n",
    "       - If valid actions exist away from seeker, chooses one\n",
    "       - Otherwise, chooses random valid action\n",
    "     - Otherwise:\n",
    "       - Computes $Q_{\\text{sum}} = Q_{1}^{\\text{target}}[x,y] + Q_{2}^{\\text{target}}[x,y]$\n",
    "       - Boosts Q-value for away direction by `tools_cfg.survive_reward`\n",
    "       - Masks invalid actions with $-\\infty$\n",
    "       - Returns $\\arg\\max_a Q_{\\text{sum}}[a]$\n",
    "   - Otherwise:\n",
    "     - Uses default `select_action_double` for normal exploration\n",
    "   - Calls `decay_epsilon()`\n",
    "   - Returns the chosen action  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *update(self, ps, a, ns, r, bps, bns)*  \n",
    "   - Delegates to `double_q_update(ps,a,ns,r,bps,bns)`\n",
    "   - This method:\n",
    "     - Adds intrinsic reward for exploration\n",
    "     - Stores experience in replay buffer\n",
    "     - Updates target networks periodically\n",
    "     - Samples batch from replay buffer\n",
    "     - Updates Q-values using double Q-learning\n",
    "     - Updates priorities based on TD errors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e83a3ddf-0057-4c84-9cca-381bff781256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLHiderAgent(AgentBase, DoubleQLearningMixin):\n",
    "    \"\"\"\n",
    "    Hider uses Double Q-learning to avoid the nearest seeker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aid: int):\n",
    "        AgentBase.__init__(self, aid)\n",
    "        DoubleQLearningMixin.__init__(self)\n",
    "\n",
    "    def select_action(self, obs, shared: SharedMemory, t: int, agents: List[AgentBase]) -> int:\n",
    "        ps = self.state()\n",
    "        seekers = [ag for ag in agents if ag.role == 'seeker']\n",
    "        \n",
    "        # Get valid actions\n",
    "        valid = []\n",
    "        for i, (dx, dy) in enumerate(actions):\n",
    "            nx, ny = ps[0] + dx, ps[1] + dy\n",
    "            if (0 <= nx < tools_cfg.grid_size and \n",
    "                0 <= ny < tools_cfg.grid_size and \n",
    "                self.env.grid[nx, ny] == 0):\n",
    "                valid.append(i)\n",
    "        \n",
    "        if not valid:\n",
    "            self.last_action = num_actions - 1\n",
    "            return num_actions - 1\n",
    "            \n",
    "        # Find nearest seeker\n",
    "        if seekers:\n",
    "            nearest = min(seekers, key=lambda s: abs(s.x - ps[0]) + abs(s.y - ps[1]))\n",
    "            dx, dy = ps[0] - nearest.x, ps[1] - nearest.y\n",
    "            away_dir = compute_bdir(dx, dy)\n",
    "            \n",
    "            # If seeker is close, prioritize moving away\n",
    "            if abs(nearest.x - ps[0]) + abs(nearest.y - ps[1]) <= 3:\n",
    "                if random.random() < self.epsilon:\n",
    "                    valid_away = [i for i in valid if i == away_dir]\n",
    "                    if valid_away:\n",
    "                        self.last_action = valid_away[0]\n",
    "                        self.decay_epsilon()\n",
    "                        return self.last_action\n",
    "                    self.last_action = random.choice(valid)\n",
    "                    self.decay_epsilon()\n",
    "                    return self.last_action\n",
    "                \n",
    "                qsum = self.Q1[ps[0], ps[1]] + self.Q2[ps[0], ps[1]]\n",
    "                # Boost Q-values for actions moving away from nearest seeker\n",
    "                qsum[away_dir] += tools_cfg.survive_reward\n",
    "                \n",
    "                masked_q = qsum.copy()\n",
    "                masked_q[~np.isin(np.arange(num_actions), valid)] = -float('inf')\n",
    "                self.last_action = int(np.argmax(masked_q))\n",
    "                return self.last_action\n",
    "        \n",
    "        # Default to normal action selection if no immediate threat\n",
    "        act = self.select_action_double(ps[0], ps[1], 0)\n",
    "        self.decay_epsilon()\n",
    "        return act\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        ps: Tuple[int, int],\n",
    "        a: int,\n",
    "        ns: Tuple[int, int],\n",
    "        r: float,\n",
    "        bps: int,\n",
    "        bns: int,\n",
    "    ):\n",
    "        self.double_q_update(ps, a, ns, r, bps, bns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ea6f-2ceb-464d-af4f-c5348b714db7",
   "metadata": {},
   "source": [
    "### ActorCriticAgent Class\n",
    "\n",
    "The `ActorCriticAgent` implements an actor-critic policy without Q-tables:\n",
    "\n",
    "1. **Policy preferences**  \n",
    "   Array $\\mathbf{H}\\in\\mathbb{R}^{N\\times N\\times A}$.\n",
    "\n",
    "2. **Value function**  \n",
    "   Array $\\mathbf{V}\\in\\mathbb{R}^{N\\times N}$.\n",
    "\n",
    "3. **Last action probabilities**  \n",
    "   Vector $\\pi\\in\\mathbb{R}^{A}$.\n",
    "\n",
    "4. **Learning rates**  \n",
    "   Scalars $\\alpha_{\\text{actor}},\\alpha_{\\text{critic}}$.\n",
    "\n",
    "5. **Vision range**  \n",
    "   Manhattan distance of $2$ for seekers and $3$ for hiders.\n",
    "\n",
    "6. **Exploration rate**  \n",
    "   Scalar $\\epsilon$ for epsilon-greedy action selection.\n",
    "\n",
    "7. **Last action**  \n",
    "   Integer tracking the most recent action taken.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self, aid: int)*  \n",
    "   - Initializes base agent and sets up actor-critic components\n",
    "   - Sets vision range based on role (2 for seeker, 3 for hider)\n",
    "   - Initialises exploration rate to `tools_cfg.epsilon_start`  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *decay_epsilon(self)*  \n",
    "   - Updates exploration rate: $\\epsilon \\gets \\max(\\epsilon_{\\min}, \\epsilon \\times \\epsilon_{\\text{decay}})$  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *select_action(self, obs, shared, t, agents)* → *int*  \n",
    "   - Gets valid actions within grid bounds\n",
    "   - If no valid actions, returns stay action\n",
    "   - If seeker and confident belief exists:\n",
    "     - Boosts preferences toward hider location\n",
    "   - If hider and nearby seekers:\n",
    "     - Boosts preferences away from nearest seeker\n",
    "   - With probability $\\epsilon$:\n",
    "     - Returns random valid action\n",
    "   - Otherwise:\n",
    "     - Computes softmax over preferences\n",
    "     - Samples action from distribution\n",
    "   - Stores probabilities and returns action  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *update(self, ps, a, ns, r)*  \n",
    "   - Adds intrinsic reward for exploration\n",
    "   - Computes TD error: $\\delta = r + \\gamma V[n_x,n_y] - V[x,y]$\n",
    "   - Updates value function: $V[x,y] \\mathrel{+}= \\alpha_{\\text{critic}}\\delta$\n",
    "   - Updates policy preferences:\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "       H[x,y,a] &\\mathrel{+}= \\alpha_{\\text{actor}}\\delta(1 - \\pi[a]) \\\\\n",
    "       H[x,y,i] &\\mathrel{-}= \\alpha_{\\text{actor}}\\delta\\,\\pi[i] \\quad \\text{for} \\quad i\\neq a\n",
    "     \\end{aligned}\n",
    "     $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "16f5cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    An agent implementing an Actor-Critic algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aid: int):\n",
    "        AgentBase.__init__(self, aid)\n",
    "        self.H = np.zeros((tools_cfg.grid_size, tools_cfg.grid_size, num_actions))\n",
    "        self.V = np.zeros((tools_cfg.grid_size, tools_cfg.grid_size))\n",
    "        self.last_probs = np.ones(num_actions) / num_actions\n",
    "        self.epsilon = tools_cfg.epsilon_start\n",
    "        self.last_action = None\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(tools_cfg.epsilon_min, self.epsilon * tools_cfg.epsilon_decay)\n",
    "\n",
    "    def select_action(self, obs, shared: SharedMemory, t: int, agents: List[AgentBase]) -> int:\n",
    "        x, y = self.state()\n",
    "        valid = []\n",
    "        for i, (dx, dy) in enumerate(actions):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if (0 <= nx < tools_cfg.grid_size and \n",
    "                0 <= ny < tools_cfg.grid_size and \n",
    "                self.env.grid[nx, ny] == 0):\n",
    "                valid.append(i)\n",
    "                \n",
    "        if not valid:\n",
    "            self.last_action = num_actions - 1\n",
    "            return num_actions - 1\n",
    "            \n",
    "        # Get observation-based preferences\n",
    "        if self.role == 'seeker':\n",
    "            belief = shared.get_belief(t, top_n=1)\n",
    "            if belief and belief[0][1] > 0.5:\n",
    "                ref = belief[0][0]\n",
    "                dx, dy = ref[0] - x, ref[1] - y\n",
    "                pref_dir = compute_bdir(dx, dy)\n",
    "                self.H[x, y, pref_dir] += tools_cfg.explore_reward\n",
    "        else:  # hider\n",
    "            seekers = [ag for ag in agents if ag.role == 'seeker']\n",
    "            if seekers:\n",
    "                nearest = min(seekers, key=lambda s: abs(s.x - x) + abs(s.y - y))\n",
    "                if abs(nearest.x - x) + abs(nearest.y - y) <= 3:\n",
    "                    dx, dy = x - nearest.x, y - nearest.y\n",
    "                    away_dir = compute_bdir(dx, dy)\n",
    "                    self.H[x, y, away_dir] += tools_cfg.survive_reward\n",
    "            \n",
    "        if random.random() < self.epsilon:\n",
    "            self.decay_epsilon()\n",
    "            self.last_action = random.choice(valid)\n",
    "            return self.last_action\n",
    "            \n",
    "        prefs = self.H[x, y].copy()\n",
    "        prefs[~np.isin(np.arange(num_actions), valid)] = -float('inf')\n",
    "        exp_prefs = np.exp(prefs - np.max(prefs))\n",
    "        probs = exp_prefs / exp_prefs.sum()\n",
    "        \n",
    "        self.last_action = np.random.choice(range(num_actions), p=probs)\n",
    "        self.last_probs = probs\n",
    "        return self.last_action\n",
    "\n",
    "    def update(\n",
    "        self, ps: Tuple[int, int], a: int, ns: Tuple[int, int], r: float\n",
    "    ):\n",
    "        # Add intrinsic reward for exploration\n",
    "        if self.env and self.env.shared:\n",
    "            visit_count = self.env.shared.record_visit(self.id, ns)\n",
    "            r += tools_cfg.beta_intrinsic / np.sqrt(visit_count + 1)\n",
    "            \n",
    "        x, y = ps\n",
    "        nx, ny = ns\n",
    "        td_err = r + tools_cfg.gamma * self.V[nx, ny] - self.V[x, y]\n",
    "        self.V[x, y] += tools_cfg.alpha_critic * td_err\n",
    "        for ai in range(num_actions):\n",
    "            if ai == a:\n",
    "                self.H[x, y, ai] += tools_cfg.alpha_actor * td_err * (1 - self.last_probs[ai])\n",
    "            else:\n",
    "                self.H[x, y, ai] -= tools_cfg.alpha_actor * td_err * self.last_probs[ai]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac8f6a",
   "metadata": {},
   "source": [
    "### Logger Class\n",
    "\n",
    "The `Logger` class records and visualises learning metrics across episodes. It maintains the following state variables:\n",
    "\n",
    "1. **Agent types**  \n",
    "   A Python `dict` mapping each agent's `id` to `\"ActorCritic\"` or `\"DoubleQ\"`.\n",
    "\n",
    "2. **Episode rewards**  \n",
    "   A Python `dict` of lists: `episode_rewards[aid]` accumulates each agent's total reward per episode.\n",
    "\n",
    "3. **Rewards by type**  \n",
    "   Two lists `rewards_by_type[\"ActorCritic\"]` and `rewards_by_type[\"DoubleQ\"]` holding average reward per episode.\n",
    "\n",
    "4. **Capture times**  \n",
    "   A list `capture_times` of integers recording how many steps each episode took to catch the hider.\n",
    "\n",
    "5. **Coverage rates**  \n",
    "   A list `coverage_rates` of floats measuring fraction of free cells explored:  \n",
    "   $$\n",
    "     \\text{coverage}_e \\;=\\; \\frac{|\\text{env.shared.explored}|}{N^2 \\;-\\;\\sum_{i,j}G_{ij}}\n",
    "   $$\n",
    "\n",
    "6. **Communication counts**  \n",
    "   A list `comm_counts` of total communication events (messages sent + received) per episode.\n",
    "\n",
    "7. **Visit counts**  \n",
    "   A NumPy array `visit_counts` of shape $(A,N,N)$ tallying how often each agent visits each grid cell.\n",
    "\n",
    "8. **Grace periods**  \n",
    "   An optional list `grace_periods` tracking the environment's `grace_counter` per episode.\n",
    "\n",
    "9. **Figures**  \n",
    "   A list `figures` storing matplotlib `Figure` objects produced by `plot`.\n",
    "\n",
    "10. **Episode actions**  \n",
    "    A list of lists tracking all actions taken in each episode.\n",
    "\n",
    "11. **Episode captures**  \n",
    "    A list tracking when captures occur in each episode.\n",
    "\n",
    "12. **Episode switch counts**  \n",
    "    A list tracking role switches per episode.\n",
    "\n",
    "13. **Reward variance**  \n",
    "    A list tracking variance in rewards across agents per episode.\n",
    "\n",
    "14. **Action variance**  \n",
    "    A list tracking variance in action selection per episode.\n",
    "\n",
    "15. **Learning progress**  \n",
    "    A list tracking Q-value convergence over time.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "1. *__init__(self, grid_size: int, agents: List[AgentBase])*  \n",
    "   - Initialises all tracking containers\n",
    "   - Sets up agent type mapping\n",
    "   - Creates visit count arrays\n",
    "   - Initialises episode counters  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "2. *log_step(self, env, agents: List[AgentBase])*  \n",
    "   - Records actions for current step\n",
    "   - Updates explored cells tracking\n",
    "   - Checks for and records captures\n",
    "   - Updates visit counts for all agents  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "3. *log_episode(self, ep: int, agents: List[AgentBase], env: HideSeekEnv)*  \n",
    "   - Records episode-level metrics:\n",
    "     - Rewards by agent and type\n",
    "     - Capture times and role switches\n",
    "     - Coverage and communication rates\n",
    "     - Action and reward variances\n",
    "     - Learning progress indicators\n",
    "   - Updates visit counts and exploration tracking\n",
    "   - Clears step-level buffers for next episode  \n",
    "    <div style=\"margin-bottom:1em;\"></div>\n",
    "\n",
    "4. *plot(self)*  \n",
    "   - Generates multiple visualization figures:\n",
    "     1. **Reward Analysis**\n",
    "        - Smoothed average rewards by agent type\n",
    "        - Reward variance over time\n",
    "        - Role-specific performance metrics\n",
    "     2. **Learning Progress**\n",
    "        - Q-value convergence\n",
    "        - Action selection patterns\n",
    "        - Exploration efficiency\n",
    "     3. **Game Dynamics**\n",
    "        - Capture times and success rates\n",
    "        - Coverage rates and exploration patterns\n",
    "        - Communication frequency\n",
    "     4. **Agent Behavior**\n",
    "        - Visit heatmaps for each agent\n",
    "        - Action distribution analysis\n",
    "        - Role switch patterns\n",
    "   - Stores all figures in `self.figures`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "72260119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"\n",
    "    Records and plots training metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size: int, agents: List[AgentBase]):\n",
    "        self.agent_types = {ag.id: ('ActorCritic' if isinstance(ag, ActorCriticAgent) else 'DoubleQ') for ag in agents}\n",
    "        self.episode_rewards: Dict[int, List[float]] = {aid: [] for aid in self.agent_types}\n",
    "        self.rewards_by_type: Dict[str, List[float]] = {'ActorCritic': [], 'DoubleQ': []}\n",
    "        self.capture_times: List[int] = []  # one entry per episode\n",
    "        self.coverage_rates: List[float] = []\n",
    "        self.comm_counts: List[int] = []\n",
    "        self.visit_counts = np.zeros((len(agents), grid_size, grid_size), dtype=int)\n",
    "        self.last_episode_visits = np.zeros((len(agents), grid_size, grid_size), dtype=int)\n",
    "        self.grace_periods: List[int] = []\n",
    "        self.figures = []\n",
    "        self.episode_count = 0\n",
    "        self.moves_per_episode: List[int] = []\n",
    "        self.stay_actions_per_episode: List[int] = []\n",
    "        self.blocked_moves_per_episode: List[int] = []\n",
    "        self.total_actions_per_episode: List[int] = []\n",
    "        self.current_episode_moves = 0\n",
    "        self.avg_q_values: List[float] = []\n",
    "        self.avg_values: List[float] = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_captures = []  # Track captures per episode\n",
    "        self.episode_switch_counts = []  # Track role switches per episode\n",
    "        self.episode_reward_variance = []  # Track reward variance per episode\n",
    "        self.episode_action_variance = []  # Track action variance per episode\n",
    "        self.successful_captures = []  # Track successful captures\n",
    "        self.avg_capture_time = []  # Track average time to capture\n",
    "        self.exploration_efficiency = []  # Track how quickly agents explore\n",
    "        self.episode_lengths = []  # Track how long episodes last\n",
    "        self.learning_progress = []  # Track Q-value changes\n",
    "        self.episode_explored_cells = []  # Track explored cells per episode\n",
    "\n",
    "    def log_step(self, env, agents: List[AgentBase]):\n",
    "        # Record all agents' actions for this step\n",
    "        step_actions = []\n",
    "        for agent in agents:\n",
    "            step_actions.append((agent.last_action, agent.moved_last_action))\n",
    "        if len(self.episode_actions) == 0:\n",
    "            self.episode_actions.append([])\n",
    "        self.episode_actions[-1].append(step_actions)\n",
    "        \n",
    "        # Track explored cells for this episode\n",
    "        if len(self.episode_explored_cells) == 0:\n",
    "            self.episode_explored_cells.append(set())\n",
    "        self.episode_explored_cells[-1].update(env.shared.explored)\n",
    "        \n",
    "        # Check for captures in this step\n",
    "        hider = next((a for a in agents if a.role == 'hider'), None)\n",
    "        if hider:\n",
    "            for agent in agents:\n",
    "                if agent.role == 'seeker' and abs(agent.x - hider.x) + abs(agent.y - hider.y) <= 1:\n",
    "                    if not self.episode_captures or self.episode_captures[-1] != env.time:\n",
    "                        self.episode_captures.append(env.time)\n",
    "                        break  \n",
    "\n",
    "    def log_episode(self, ep: int, agents: List[AgentBase], env: HideSeekEnv):\n",
    "        if self.episode_captures:\n",
    "            first_cap = self.episode_captures[0]\n",
    "        else:\n",
    "            first_cap = tools_cfg.t_max\n",
    "        self.capture_times.append(first_cap)\n",
    "        self.episode_count = ep\n",
    "        self.last_episode_visits.fill(0)\n",
    "\n",
    "        self.episode_switch_counts.append(env.switch_count)\n",
    "        \n",
    "        episode_rewards = [ag.reward_sum for ag in agents]\n",
    "        self.episode_reward_variance.append(np.var(episode_rewards))\n",
    "        \n",
    "        action_counts = np.zeros(num_actions)\n",
    "        for step_actions in self.episode_actions[-1]:\n",
    "            for action, _ in step_actions:\n",
    "                action_counts[action] += 1\n",
    "        self.episode_action_variance.append(np.var(action_counts))\n",
    "        \n",
    "        total_free = tools_cfg.grid_size ** 2 - env.grid.sum()\n",
    "        # Use the episode-specific explored cells\n",
    "        episode_coverage = len(self.episode_explored_cells[-1]) / total_free\n",
    "        self.coverage_rates.append(episode_coverage)\n",
    "        \n",
    "        # Start a new set for the next episode\n",
    "        self.episode_explored_cells.append(set())\n",
    "        \n",
    "        total_moves = 0\n",
    "        total_stays = 0\n",
    "        total_blocked = 0\n",
    "        total_actions = 0\n",
    "        total_q = 0\n",
    "        total_v = 0\n",
    "        q_count = 0\n",
    "        v_count = 0\n",
    "        \n",
    "        for ag in agents:\n",
    "            self.episode_rewards[ag.id].append(ag.reward_sum)\n",
    "            x, y = ag.state()\n",
    "            self.visit_counts[ag.id, x, y] += 1\n",
    "            self.last_episode_visits[ag.id, x, y] += 1\n",
    "            \n",
    "            if isinstance(ag, DoubleQLearningMixin):\n",
    "                total_q += np.mean(ag.Q1 + ag.Q2)\n",
    "                q_count += 1\n",
    "            elif isinstance(ag, ActorCriticAgent):\n",
    "                total_v += np.mean(ag.V)\n",
    "                v_count += 1\n",
    "\n",
    "        if q_count > 0:\n",
    "            self.avg_q_values.append(total_q / q_count)\n",
    "        if v_count > 0:\n",
    "            self.avg_values.append(total_v / v_count)\n",
    "\n",
    "        for typ in self.rewards_by_type:\n",
    "            ids = [aid for aid, t in self.agent_types.items() if t == typ]\n",
    "            vals = [self.episode_rewards[aid][ep] for aid in ids]\n",
    "            avg = float(np.mean(vals)) if vals else 0.0\n",
    "            self.rewards_by_type[typ].append(avg)\n",
    "\n",
    "        total_free = tools_cfg.grid_size ** 2 - env.grid.sum()\n",
    "        coverage = len(env.shared.explored) / total_free\n",
    "        self.coverage_rates.append(coverage)\n",
    "        self.comm_counts.append(env.shared.total_comm_count)\n",
    "        if hasattr(env, 'grace_counter'):\n",
    "            self.grace_periods.append(env.grace_counter)\n",
    "\n",
    "        # Count all actions from the episode_actions list\n",
    "        for step_actions in self.episode_actions[-1]:  \n",
    "            for action, moved in step_actions:\n",
    "                total_actions += 1\n",
    "                if action == num_actions - 1:  # Stay action\n",
    "                    total_stays += 1\n",
    "                elif not moved:  \n",
    "                    total_blocked += 1\n",
    "                else:  # Successful move\n",
    "                    total_moves += 1\n",
    "        \n",
    "        self.moves_per_episode.append(total_moves)\n",
    "        self.stay_actions_per_episode.append(total_stays)\n",
    "        self.blocked_moves_per_episode.append(total_blocked)\n",
    "        self.total_actions_per_episode.append(total_actions)\n",
    "        \n",
    "        self.episode_actions.append([])\n",
    "\n",
    "        self.episode_captures.clear()\n",
    "\n",
    "        # Track successful captures\n",
    "        if env.switch_count > 0:\n",
    "            self.successful_captures.append(1)\n",
    "        else:\n",
    "            self.successful_captures.append(0)\n",
    "            \n",
    "        # Track average capture time\n",
    "        if self.episode_captures:\n",
    "            self.avg_capture_time.append(np.mean(self.episode_captures))\n",
    "        else:\n",
    "            self.avg_capture_time.append(tools_cfg.t_max)\n",
    "            \n",
    "        # Track exploration efficiency\n",
    "        self.exploration_efficiency.append(coverage)\n",
    "        \n",
    "        # Track episode length\n",
    "        self.episode_lengths.append(env.time)\n",
    "        \n",
    "        # Track learning progress (Q-value changes)\n",
    "        q_changes = []\n",
    "        for ag in agents:\n",
    "            if isinstance(ag, DoubleQLearningMixin):\n",
    "                q_changes.append(np.mean(np.abs(ag.Q1 - ag.Q2)))\n",
    "        if q_changes:\n",
    "            self.learning_progress.append(np.mean(q_changes))\n",
    "\n",
    "    def plot(self):\n",
    "        # Create a figure for rewards and moves\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Smoothed reward\n",
    "        df = pd.DataFrame(self.rewards_by_type)\n",
    "        df.rolling(window=50).mean().plot(ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Smoothed Avg Reward per Agent Type')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot 2: Movement statistics\n",
    "        moves_df = pd.DataFrame({\n",
    "            'Successful Moves': self.moves_per_episode,\n",
    "            'Stay Actions': self.stay_actions_per_episode,\n",
    "            'Blocked Moves': self.blocked_moves_per_episode\n",
    "        })\n",
    "        moves_df.rolling(window=50).mean().plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Movement Statistics per Episode')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot 3: Q-values\n",
    "        if self.avg_q_values:\n",
    "            q_series = pd.Series(self.avg_q_values)\n",
    "            q_series.rolling(window=50).mean().plot(ax=axes[1, 0], label='Q-values')\n",
    "            axes[1, 0].set_title('Smoothed Average Q-values')\n",
    "            axes[1, 0].set_xlabel('Episode')\n",
    "            axes[1, 0].set_ylabel('Average Q-value')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot 4: Reward Variance\n",
    "        reward_variance = pd.DataFrame(self.rewards_by_type).rolling(window=50).var()\n",
    "        reward_variance.plot(ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Rolling Variance of Rewards (50-episode window)')\n",
    "        axes[1, 1].set_xlabel('Episode')\n",
    "        axes[1, 1].set_ylabel('Variance')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)\n",
    "\n",
    "        # Create a figure for capture times and learning metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot capture times (actual captures)\n",
    "        capture_series = pd.Series(self.capture_times)\n",
    "        capture_series.rolling(window=50).mean().plot(ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Average Time to Capture')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Time Steps')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot role switches over time\n",
    "        switch_series = pd.Series(self.episode_switch_counts)\n",
    "        switch_series.rolling(window=50).mean().plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Role Switches per Episode')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Number of Switches')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot action variance over time\n",
    "        action_var_series = pd.Series(self.episode_action_variance)\n",
    "        action_var_series.rolling(window=50).mean().plot(ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Action Selection Variance')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Variance')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot reward variance over time\n",
    "        reward_var_series = pd.Series(self.episode_reward_variance)\n",
    "        reward_var_series.rolling(window=50).mean().plot(ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Episode Reward Variance')\n",
    "        axes[1, 1].set_xlabel('Episode')\n",
    "        axes[1, 1].set_ylabel('Variance')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)\n",
    "\n",
    "        # Create a figure for capture times and coverage\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # Plot capture times\n",
    "        capture_series = pd.Series(self.capture_times)\n",
    "        capture_series.rolling(window=50).mean().plot(ax=axes[0])\n",
    "        axes[0].set_title('Smoothed Capture Time per Episode')\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Time Steps')\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Plot coverage rate\n",
    "        coverage_series = pd.Series(self.coverage_rates)\n",
    "        coverage_series.rolling(window=50).mean().plot(ax=axes[1])\n",
    "        axes[1].set_title('Smoothed Coverage Rate per Episode')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Coverage Rate')\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)\n",
    "\n",
    "        # Create a figure for learning metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Success Rate\n",
    "        success_rate = pd.Series(self.successful_captures).rolling(window=50).mean()\n",
    "        success_rate.plot(ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Capture Success Rate (50-episode window)')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Success Rate')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot 2: Average Capture Time\n",
    "        # Only consider episodes where a capture occurred\n",
    "        capture_times = [t for t in self.capture_times if t < tools_cfg.t_max]\n",
    "        if capture_times:\n",
    "            capture_time = pd.Series(capture_times).rolling(window=50).mean()\n",
    "            capture_time.plot(ax=axes[0, 1])\n",
    "            axes[0, 1].set_title('Average Time to Capture (only successful captures)')\n",
    "            axes[0, 1].set_xlabel('Episode')\n",
    "            axes[0, 1].set_ylabel('Time Steps')\n",
    "            axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot 3: Exploration Efficiency\n",
    "        exploration = pd.Series(self.exploration_efficiency).rolling(window=50).mean()\n",
    "        exploration.plot(ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Exploration Efficiency')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Coverage Rate')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot 4: Q-value Convergence\n",
    "        if self.learning_progress:\n",
    "            learning = pd.Series(self.learning_progress).rolling(window=50).mean()\n",
    "            learning.plot(ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Q-value Convergence')\n",
    "            axes[1, 1].set_xlabel('Episode')\n",
    "            axes[1, 1].set_ylabel('Q-value Difference')\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)\n",
    "\n",
    "        # Create a figure for reward and action statistics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Reward Variance\n",
    "        reward_variance = pd.DataFrame(self.rewards_by_type).rolling(window=50).var()\n",
    "        reward_variance.plot(ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Rolling Variance of Rewards (50-episode window)')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Variance')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot 2: Role Switches\n",
    "        switch_series = pd.Series(self.episode_switch_counts)\n",
    "        switch_series.rolling(window=50).mean().plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Role Switches per Episode')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Number of Switches')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot 3: Episode Lengths\n",
    "        length_series = pd.Series(self.episode_lengths)\n",
    "        length_series.rolling(window=50).mean().plot(ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Average Episode Length')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Time Steps')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot 4: Action Variance\n",
    "        action_var_series = pd.Series(self.episode_action_variance)\n",
    "        action_var_series.rolling(window=50).mean().plot(ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Action Selection Variance')\n",
    "        axes[1, 1].set_xlabel('Episode')\n",
    "        axes[1, 1].set_ylabel('Variance')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412bcbc-bc22-4804-92ef-06b91d184d35",
   "metadata": {},
   "source": [
    "# 4. Choice & Description of Data  \n",
    "\n",
    "In this work, we employ a fully synthetic, grid-based environment to isolate and evaluate multi-agent hide-and-seek strategies under controlled complexity. All parameters are configurable, allowing systematic study of agent behaviour as environment properties vary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b5dde-eb2f-4f9c-b576-6997024733c3",
   "metadata": {},
   "source": [
    "### Training and Demo Function\n",
    "\n",
    "The `run_training_and_demo()` function orchestrates the complete training process for the hide-and-seek environment. It maintains the following key components:\n",
    "\n",
    "1. **Environment Setup**  \n",
    "   - Creates `HideSeekEnv` instance\n",
    "   - Initialises agents with specific roles:\n",
    "     - Agent 0: `RLSeekerAgent`\n",
    "     - Agent 1: `ActorCriticAgent`\n",
    "     - Remaining agents: `RLHiderAgent`\n",
    "\n",
    "2. **Performance Tracking**  \n",
    "   - Dictionary `episode_rewards` tracking rewards per agent\n",
    "   - Dictionary `best_rewards` storing best performance\n",
    "   - Dictionary `no_improvement_count` tracking learning plateaus\n",
    "\n",
    "3. **Logger**  \n",
    "   - `Logger` instance for metric collection and visualization\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Multi-Agent Training**  \n",
    "   - Coordinates multiple agents with different learning algorithms\n",
    "   - Handles role-specific updates and rewards\n",
    "\n",
    "2. **Adaptive Learning**  \n",
    "   - Adjusts learning rates based on performance\n",
    "   - Tracks learning plateaus\n",
    "   - Implements intrinsic rewards for exploration\n",
    "\n",
    "3. **Comprehensive Logging**  \n",
    "   - Records detailed metrics at step and episode levels\n",
    "   - Tracks performance across different agent types\n",
    "   - Generates visualisation plots\n",
    "\n",
    "4. **Role-Specific Processing**  \n",
    "   - Different update logic for seeker vs hider agents\n",
    "   - Belief-based updates for seekers\n",
    "   - Evasion-based updates for hiders\n",
    "\n",
    "5. **Performance Optimisation**  \n",
    "   - Tracks best performance per agent\n",
    "   - Monitors learning progress\n",
    "   - Adapts learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "a50d33df-8c22-4d33-93cf-2092bd284ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Training & Demo ----------------------\n",
    "def run_training_and_demo():\n",
    "    \"\"\"\n",
    "    Initialise environment, create agents, run training for num_episodes,\n",
    "    log metrics and plot results.\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing environment and agents...\")\n",
    "    env = HideSeekEnv()\n",
    "    agents: List[AgentBase] = []\n",
    "    for aid in range(tools_cfg.num_agents):\n",
    "        if aid == 0:\n",
    "            ag = RLSeekerAgent(aid)\n",
    "        elif aid == 1:\n",
    "            ag = ActorCriticAgent(aid)\n",
    "        else:\n",
    "            ag = RLHiderAgent(aid)\n",
    "        env.add_agent(ag, (0, 0))\n",
    "        agents.append(ag)\n",
    "    logger = Logger(tools_cfg.grid_size, agents)\n",
    "\n",
    "    # Performance tracking\n",
    "    episode_rewards = {aid: [] for aid in range(tools_cfg.num_agents)}\n",
    "    best_rewards = {aid: float('-inf') for aid in range(tools_cfg.num_agents)}\n",
    "    no_improvement_count = {aid: 0 for aid in range(tools_cfg.num_agents)}\n",
    "    \n",
    "    print(f\"\\nStarting training for {tools_cfg.num_episodes} episodes...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for ep in range(tools_cfg.num_episodes):\n",
    "        env.reset()\n",
    "        for ag in agents:\n",
    "            ag.visited.clear()\n",
    "        logger.episode_actions.append([])\n",
    "        \n",
    "        episode_reward = {aid: 0.0 for aid in range(tools_cfg.num_agents)}\n",
    "        \n",
    "        for t in range(tools_cfg.t_max):\n",
    "            prev_states = {ag.id: ag.state() for ag in agents}\n",
    "            actions_dict = {\n",
    "                ag.id: ag.select_action(None, env.shared, env.time, agents)\n",
    "                for ag in agents\n",
    "            }\n",
    "            rewards = env.step(actions_dict)\n",
    "            logger.log_step(env, agents)\n",
    "\n",
    "            for ag in agents:\n",
    "                ps, ns = prev_states[ag.id], ag.state()\n",
    "                r = rewards[ag.id]\n",
    "                episode_reward[ag.id] += r\n",
    "                \n",
    "                if tools_cfg.use_intrinsic and ag.role == 'seeker':\n",
    "                    cnt = env.shared.record_visit(ag.id, ns)\n",
    "                    r += tools_cfg.beta_intrinsic / np.sqrt(cnt + 1)\n",
    "\n",
    "                if isinstance(ag, ActorCriticAgent):\n",
    "                    ag.update(ps, actions_dict[ag.id], ns, r)\n",
    "                else:\n",
    "                    if isinstance(ag, RLSeekerAgent):\n",
    "                        belief_pre = env.shared.get_belief(env.time, top_n=1)\n",
    "                        ref_pre = belief_pre[0][0] if belief_pre else ps\n",
    "                        bps = compute_bdir(ref_pre[0] - ps[0], ref_pre[1] - ps[1])\n",
    "                        belief_post = env.shared.get_belief(env.time, top_n=1)\n",
    "                        ref_post = belief_post[0][0] if belief_post else ns\n",
    "                        bns = compute_bdir(ref_post[0] - ns[0], ref_post[1] - ns[1])\n",
    "                    else:\n",
    "                        bps, bns = 0, 0\n",
    "                    ag.update(ps, actions_dict[ag.id], ns, r, bps, bns)\n",
    "                ag.reward_sum += r\n",
    "\n",
    "        # Update learning rates based on performance\n",
    "        for ag in agents:\n",
    "            if isinstance(ag, DoubleQLearningMixin):\n",
    "                ag.update_learning_rate(episode_reward[ag.id])\n",
    "            \n",
    "            # Track best performance\n",
    "            if episode_reward[ag.id] > best_rewards[ag.id]:\n",
    "                best_rewards[ag.id] = episode_reward[ag.id]\n",
    "                no_improvement_count[ag.id] = 0\n",
    "            else:\n",
    "                no_improvement_count[ag.id] += 1\n",
    "                \n",
    "            episode_rewards[ag.id].append(episode_reward[ag.id])\n",
    "                \n",
    "        logger.log_episode(ep, agents, env)\n",
    "        \n",
    "        if (ep + 1) % 100 == 0:\n",
    "            print(f\"Episode {ep + 1}/{tools_cfg.num_episodes} completed\")\n",
    "\n",
    "    print(\"\\nTraining completed. Generating plots...\")\n",
    "    logger.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63ec6d-d1ee-4494-b7d5-d8a97744da68",
   "metadata": {},
   "source": [
    "### TestCoreExtended Class\n",
    "\n",
    "The `TestCoreExtended` class verifies the environment is created properly and handles edge cases well. This would give us confident in our complex learning algorithms implemented.\n",
    "\n",
    "_setUp_\n",
    " - Initialises a fresh environment with a seeker and hider in known positions for consistent testing conditions.\n",
    "\n",
    "_test_seeker_sees_hider_updates_belief_\n",
    " - Verifies that seeker sightings correctly update the shared belief system with proper weight distribution.\n",
    "\n",
    "_test_capture_does_not_update_belief_\n",
    " - Ensures belief system maintains appropriate uncertainty levels after hider captures.\n",
    "\n",
    "_test_invisible_hider_not_seen_\n",
    " - Confirms that seekers cannot detect hiders during their grace period after role switches.\n",
    "\n",
    "_test_role_switch_logic_\n",
    " - Validates correct role swapping and grace period application after successful captures.\n",
    "\n",
    "_test_intrinsic_reward_added_for_seekers_\n",
    " - Tests the exploration bonus system for seekers, ensuring proper reward calculation and application.\n",
    "\n",
    "_test_full_episode_runs_t_max_steps_\n",
    " - Guarantees episodes run for exactly the specified maximum number of steps without premature termination.\n",
    "\n",
    "_test_training_loop_never_ends_early_\n",
    " - Ensures training episodes maintain consistent length and proper termination conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "f3356784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCoreExtended(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \"\"\"Shared setup for each test.\"\"\"\n",
    "        self.env = HideSeekEnv()\n",
    "        self.seeker = RLSeekerAgent(0)\n",
    "        self.hider = RLHiderAgent(1)\n",
    "        self.env.grid = np.zeros_like(self.env.grid)\n",
    "        self.env.add_agent(self.seeker, (5, 5))\n",
    "        self.env.add_agent(self.hider, (5, 6))\n",
    "        self.seeker.role = 'seeker'\n",
    "        self.hider.role = 'hider'\n",
    "\n",
    "    def test_seeker_sees_hider_updates_belief(self):\n",
    "        self.seeker.invisible = self.hider.invisible = 0\n",
    "        self.env.shared.clear_step()\n",
    "        self.env.shared.decay_confidence(self.env.time)\n",
    "        self.env.shared.diffuse_belief()\n",
    "        obs, seen = self.seeker.observe([self.seeker, self.hider], self.env)\n",
    "        self.assertTrue(seen)\n",
    "        self.env.shared.record_sighting(self.seeker.id, self.hider.state(), self.env.time)\n",
    "        top_loc, weight = self.env.shared.get_belief(self.env.time, top_n=1)[0]\n",
    "        self.assertEqual(top_loc, self.hider.state())\n",
    "        self.assertAlmostEqual(weight, 1.0, places=2)\n",
    "\n",
    "    def test_capture_does_not_update_belief(self):\n",
    "        self.env.shared.clear_episode()\n",
    "        self.env.shared.confidence = 0.5\n",
    "        self.env.shared.belief.fill(1 / (tools_cfg.grid_size ** 2))\n",
    "        rewards = self.env.step({self.seeker.id: 0})\n",
    "        top_loc, weight = self.env.shared.get_belief(self.env.time, top_n=1)[0]\n",
    "        self.assertLess(weight, 1.0)\n",
    "\n",
    "    def test_invisible_hider_not_seen(self):\n",
    "        self.hider.invisible = 999\n",
    "        obs, seen = self.seeker.observe([self.seeker, self.hider], self.env)\n",
    "        self.assertFalse(seen)\n",
    "\n",
    "    def test_role_switch_logic(self):\n",
    "        rewards = self.env.step({self.seeker.id: 0})\n",
    "        self.assertEqual(self.seeker.role, 'hider')\n",
    "        self.assertEqual(self.hider.role, 'seeker')\n",
    "        self.assertEqual(self.seeker.invisible, tools_cfg.grace_steps)\n",
    "\n",
    "    def test_intrinsic_reward_added_for_seekers(self):\n",
    "        self.env.reset()\n",
    "        self.seeker.x, self.seeker.y = 5, 5\n",
    "        self.hider.x, self.hider.y = 0, 0\n",
    "        prev = self.seeker.reward_sum\n",
    "        prev_pos = self.seeker.state()\n",
    "        act = self.seeker.select_action(None, self.env.shared, self.env.time, [self.seeker, self.hider])\n",
    "        rewards = self.env.step({self.seeker.id: act})\n",
    "        post_pos = self.seeker.state()\n",
    "        cnt = self.env.shared.record_visit(self.seeker.id, post_pos)\n",
    "        bonus = tools_cfg.beta_intrinsic / np.sqrt(cnt + 1)\n",
    "        self.assertGreater(rewards[self.seeker.id] + bonus, rewards[self.seeker.id])\n",
    "\n",
    "    def test_full_episode_runs_t_max_steps(self):\n",
    "        \"\"\"Ensure each episode always advances exactly t_max steps.\"\"\"\n",
    "        for _ in range(3):\n",
    "            env = HideSeekEnv()\n",
    "            agents: List[AgentBase] = []\n",
    "            for aid in range(tools_cfg.num_agents):\n",
    "                ag = [RLSeekerAgent, ActorCriticAgent, RLHiderAgent][min(aid, 2)](aid)\n",
    "                env.add_agent(ag, (0, 0))\n",
    "                agents.append(ag)\n",
    "\n",
    "            env.reset()\n",
    "            self.assertEqual(env.time, 0)\n",
    "            for _ in range(tools_cfg.t_max):\n",
    "                actions_dict = {\n",
    "                    ag.id: ag.select_action(None, env.shared, env.time, agents)\n",
    "                    for ag in agents\n",
    "                }\n",
    "                env.step(actions_dict)\n",
    "\n",
    "            self.assertEqual(\n",
    "                env.time,\n",
    "                tools_cfg.t_max,\n",
    "                f\"Expected env.time == {tools_cfg.t_max}, got {env.time}\"\n",
    "            )\n",
    "\n",
    "    def test_training_loop_never_ends_early(self):\n",
    "        \"\"\"Ensure run_training_and_demo always runs each episode for t_max steps.\"\"\"\n",
    "        # Create a small test environment\n",
    "        env = HideSeekEnv()\n",
    "        agents = []\n",
    "        for aid in range(tools_cfg.num_agents):\n",
    "            ag = [RLSeekerAgent, ActorCriticAgent, RLHiderAgent][min(aid, 2)](aid)\n",
    "            env.add_agent(ag, (0, 0))\n",
    "            agents.append(ag)\n",
    "            \n",
    "        # Run just 10 episodes to test\n",
    "        for ep in range(10):\n",
    "            env.reset()\n",
    "            for t in range(tools_cfg.t_max):\n",
    "                actions_dict = {\n",
    "                    ag.id: ag.select_action(None, env.shared, env.time, agents)\n",
    "                    for ag in agents\n",
    "                }\n",
    "                env.step(actions_dict)\n",
    "            self.assertEqual(env.time, tools_cfg.t_max, \n",
    "                           f\"Episode {ep} ended at step {env.time}, expected {tools_cfg.t_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "918743fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_capture_does_not_update_belief (__main__.TestCoreExtended.test_capture_does_not_update_belief) ... ok\n",
      "test_full_episode_runs_t_max_steps (__main__.TestCoreExtended.test_full_episode_runs_t_max_steps)\n",
      "Ensure each episode always advances exactly t_max steps. ... ok\n",
      "test_intrinsic_reward_added_for_seekers (__main__.TestCoreExtended.test_intrinsic_reward_added_for_seekers) ... ok\n",
      "test_invisible_hider_not_seen (__main__.TestCoreExtended.test_invisible_hider_not_seen) ... ok\n",
      "test_role_switch_logic (__main__.TestCoreExtended.test_role_switch_logic) ... ok\n",
      "test_seeker_sees_hider_updates_belief (__main__.TestCoreExtended.test_seeker_sees_hider_updates_belief) ... ok\n",
      "test_training_loop_never_ends_early (__main__.TestCoreExtended.test_training_loop_never_ends_early)\n",
      "Ensure run_training_and_demo always runs each episode for t_max steps. ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running main...\n",
      "Running tests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.464s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests completed, running training and demo...\n",
      "\n",
      "Initializing environment and agents...\n",
      "\n",
      "Starting training for 2000 episodes...\n",
      "================================================================================\n",
      "Episode 100/2000 completed\n",
      "Episode 200/2000 completed\n",
      "Episode 300/2000 completed\n",
      "Episode 400/2000 completed\n",
      "Episode 500/2000 completed\n",
      "Episode 600/2000 completed\n",
      "Episode 700/2000 completed\n",
      "Episode 800/2000 completed\n",
      "Episode 900/2000 completed\n",
      "Episode 1000/2000 completed\n",
      "Episode 1100/2000 completed\n",
      "Episode 1200/2000 completed\n",
      "Episode 1300/2000 completed\n",
      "Episode 1400/2000 completed\n",
      "Episode 1500/2000 completed\n",
      "Episode 1600/2000 completed\n",
      "Episode 1700/2000 completed\n",
      "Episode 1800/2000 completed\n",
      "Episode 1900/2000 completed\n",
      "Episode 2000/2000 completed\n",
      "\n",
      "Training completed. Generating plots...\n",
      "Program completed.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running main...\")\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestCoreExtended)\n",
    "    print(\"Running tests...\")\n",
    "    unittest.TextTestRunner(verbosity=2).run(suite)\n",
    "    print(\"Tests completed, running training and demo...\")\n",
    "\n",
    "run_training_and_demo()\n",
    "print(\"Program completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face930d-b1db-44d8-9c1e-f82c27ace291",
   "metadata": {},
   "source": [
    "<img src=\"training_outputs/plot_1.png\" width=\"900\"/>  \n",
    "<img src=\"training_outputs/plot_2.png\" width=\"900\"/>  \n",
    "<img src=\"training_outputs/plot_3.png\" width=\"900\"/>  \n",
    "<img src=\"training_outputs/plot_4.png\" width=\"900\"/>  \n",
    "<img src=\"training_outputs/plot_5.png\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d0dd4",
   "metadata": {},
   "source": [
    "## 5. Numerical Evaluation\n",
    "\n",
    "Multi-agent environments can be categorised as cooperative, competitive, or mixed. In mixed settings, such as ours, analysing dynamics without explicit equilibrium analysis is inherently incomplete. Nevertheless, our findings provide reasons to believe that agents in our scenario are indeed moving towards equilibrium.\n",
    "\n",
    "Firstly, Kuba et al. (2022) highlight that policy-gradient methods become less effective in multi-agent settings if opponents simultaneously update their policies, treating others merely as part of the environment. Centralised training with decentralised execution frameworks, such as Multi-Agent Proximal Policy Optimisation, typically mitigates this issue by evaluating policies across larger action samples. Although we utilised a standard policy-gradient method (actor-critic) without centralised training, convergence was still observed.\n",
    "\n",
    "This convergence is demonstrated by an increase in the average number of role switches per episode, coupled with a synchronous and gradual decline in the average capture time. Within our environment, this trend suggests that seeker agents improved coordination through information diffusion mechanisms, effectively converging towards the hider’s last known position. As episodes increased and role switching became rewarding, seekers learned more efficient movements towards the hider, thus increasing switching frequency.\n",
    "\n",
    "Additionally, the observed rise in episode reward variance underscores the reward mechanism—agents gain substantial rewards when successfully locating the hider; therefore, greater switching frequency naturally increased reward variability.\n",
    "\n",
    "For agents employing the Double Q-Learning algorithm, we also observed expected convergence of Q-values. Initially, there was a pronounced increase in Q-value differences due to the exploration of previously unseen state-action pairs. However, after approximately 600 episodes, these differences steadily diminished, indicating that sufficient states had been explored, allowing Q-values to stabilise.\n",
    "\n",
    "The per-agent plots comparing smoothed rewards indicated minimal differences between Double Q-learning and actor-critic methods, implying both algorithms adapted effectively and learned at comparable rates.\n",
    "\n",
    "Other performance metrics considered included the average number of time steps required for an agent to be captured and the generalisation capabilities of agents in new environments. However, imbalanced agent numbers across learning paradigms caused significantly higher payoff variance for actor-critic agents, rendering comparisons between paradigms unreliable, as illustrated by the Rolling Variance of Rewards plot. Additionally, the coverage of the 10x10 grid ranged only from 14% to 20%, suggesting inadequate exploration even on fixed maps with known obstacles, thereby limiting agents’ abilities to generalise effectively to new environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cee20f",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Throughout this project, we trained multiple agents within a grid world environment using two reinforcement learning methods—Actor-Critic and Double Q learning. We have analysed how these decentralised training and decentralised execution perform in a novel mixed (cooperative and competitive) multi-agent setting of interaction. Our results find that both Actor-Critic and Double Q-learning agents demonstrated effective convergence behaviours. This was seen particularly for the Double-Q-Learning agents through the constant decline in the difference between Q-values. \n",
    "\n",
    "What could be considered is dynamics that could arise in the environment beyond 2000 episodes. In particular, we would like to see if training a set map for long enough would (i) increase the agents' coverage of the map and (ii) allow agents to generalise their learning to new maps (with different sets of obstacles). Moreover, decentralised training and execution frameworks are known to converge slower to equilibria as agents sequentially respond to the previous policies of each other. An interesting question to consider is how much quicker centralised training would perform in this novel multi-agent environment. \n",
    "\n",
    "In sum, this project demonstrates that multiple agents can learn to coordinate and compete in a simple grid world. The findings highlight the importance of stability and reward design in multi-agent systems and point the way towards more robust and scalable learning strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c511e7",
   "metadata": {},
   "source": [
    "## 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2eb5b-a7e8-43ff-9faa-6d8a65891e47",
   "metadata": {},
   "source": [
    "- Kuba, Jakub Grudzien, et al. “Trust region policy optimisation in multi-agent reinforcement learning.” *arXiv preprint arXiv:2109.11251* (2021).\n",
    "\n",
    "- van Hasselt, Hado; Guez, Arthur; Silver, David. “Deep Reinforcement Learning with Double Q-learning.” *Proceedings of the AAAI Conference on Artificial Intelligence* (2016).\n",
    "\n",
    "- Sutton, Richard S.; McAllester, David A.; Singh, Satinder P.; Mansour, Yishay. “Policy gradient methods for reinforcement learning with function approximation.” *Advances in Neural Information Processing Systems* 12 (2000).\n",
    "\n",
    "- Pathak, Deepak; Agrawal, Pulkit; Efros, Alexei A.; Darrell, Trevor. “Curiosity-driven Exploration by Self-supervised Prediction.” *Proceedings of the 34th International Conference on Machine Learning* (2017).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
